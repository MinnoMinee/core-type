{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import laspy\n",
    "import matplotlib.pyplot    as plt\n",
    "import numpy                as np\n",
    "import cupy                 as cp\n",
    "import open3d               as o3d\n",
    "from scipy.stats            import skew, kurtosis\n",
    "from scipy.ndimage          import convolve\n",
    "from cupyx.scipy.ndimage    import convolve as cpconvolve\n",
    "from collections            import deque\n",
    "from scipy.interpolate      import interp1d\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.ndimage import label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lidar_processor:\n",
    "    def __init__(self, file_path, DBSCAN_model_path = None, window_size = 10, upsample_ratio = 2, y_shift = 0, name=None,  \n",
    "                strong_PCA_threshold = 0.15, weak_PCA_threshold = 0.025, strong_edge_threshold = 80, weak_edge_threshold = 30, colourmap = 'binary',\n",
    "                x_stop = 550 , box_length = 1500, xrf_window_size = 10, noise_threshold = 0.1):\n",
    "        \n",
    "        if DBSCAN_model_path is None:\n",
    "            current_dir = os.getcwd()\n",
    "            self.DBSCAN_model_path = os.path.join(current_dir, 'cluster_kd_tree.pkl')\n",
    "        else: \n",
    "            self.DBSCAN_model_path = DBSCAN_model_path\n",
    "             \n",
    "        self.file_path = file_path\n",
    "        self.colourmap = colourmap\n",
    "        self.name = name if name else file_path\n",
    "        self.upsample_ratio = upsample_ratio\n",
    "        self.window_size = window_size\n",
    "        self.strong_edge_threshold = strong_edge_threshold\n",
    "        self.weak_edge_threshold = weak_edge_threshold\n",
    "        self.strong_PCA_threshold = strong_PCA_threshold\n",
    "        self.weak_PCA_threshold = weak_PCA_threshold\n",
    "        self.y_shift = y_shift\n",
    "        self.x_stop = x_stop\n",
    "        self.x_start = self.x_stop + box_length\n",
    "        self.noise_threshold = noise_threshold\n",
    "        self.xrf_window_size = xrf_window_size\n",
    "\n",
    "        self._build_processor()\n",
    "    \n",
    "    def _collect_garbage(self):\n",
    "        self.tree = None\n",
    "        self.labels = None\n",
    "        self.correction_windows = None\n",
    "        self.labeled_x_values = None\n",
    "        self.gradient = None\n",
    "        self.PCA_mask = None\n",
    "        self.component_parameters_path = None\n",
    "        self.lidar2xrf_path = None\n",
    "        self.bpc_path = None\n",
    "        self.intensity_path = None\n",
    "        self.real_dbg_path = None\n",
    "        self.lidar_dbg_path = None\n",
    "\n",
    "    def _find_files(self):\n",
    "        self.component_parameters_path = None\n",
    "        self.lidar2xrf_path = None\n",
    "        self.bpc_path = None\n",
    "        self.intensity_path = None\n",
    "        self.real_dbg_path = None\n",
    "        self.lidar_dbg_path = None\n",
    "        for file_name in os.listdir(self.file_path):\n",
    "            full_path = os.path.join(self.file_path, file_name)\n",
    "            if file_name.endswith(\".component_parameters.txt\"):\n",
    "                self.component_parameters_path = full_path\n",
    "            elif file_name.endswith(\".lidar2xrf\"):\n",
    "                self.lidar2xrf_path = full_path\n",
    "            elif file_name.endswith(\".bpc\"):\n",
    "                self.bpc_path = full_path\n",
    "            elif file_name.endswith(\"_intensity.png\"):\n",
    "                self.intensity_path = full_path\n",
    "            elif file_name.endswith(\"real.dbg\"):\n",
    "                self.real_dbg_path = full_path\n",
    "            elif file_name.endswith(\".dbg\"):\n",
    "                self.lidar_dbg_path = full_path\n",
    "\n",
    "    def _validate_files(self):\n",
    "        missing_files = [\n",
    "            name for name, path in {\n",
    "                \"bpc\": self.bpc_path,\n",
    "                \"intensity\": self.intensity_path,\n",
    "                \"real_dbg\": self.real_dbg_path,\n",
    "                \"dbg\": self.lidar_dbg_path,\n",
    "            }.items() if path is None\n",
    "        ]\n",
    "\n",
    "        non_essential_mising_files = [\n",
    "            name for name, path in {\n",
    "                \"component_parameters\": self.component_parameters_path,\n",
    "                \"lidar2xrf\": self.lidar2xrf_path,\n",
    "            }.items() if path is None\n",
    "        ]\n",
    "\n",
    "        if missing_files:\n",
    "            raise FileNotFoundError(f\"Missing required files: {', '.join(missing_files)}\")\n",
    "        if non_essential_mising_files:\n",
    "            print (f\"   Missing: {', '.join(non_essential_mising_files)}, contents will be assumed\")\n",
    "    \n",
    "    def _interpolate_x_positions(self):\n",
    "        with open(self.real_dbg_path) as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        positions = []\n",
    "        table_timestamps = []\n",
    "\n",
    "        char = '-'\n",
    "        for line in lines:\n",
    "            if len(line) > 40:\n",
    "                if \"+\" in line:\n",
    "                    char = '+'\n",
    "                time = line.split('T')[1]\n",
    "                x = float(line.split(',')[0])\n",
    "                parts = time.split(':')\n",
    "                t = float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2].split(char)[0]) \n",
    "                positions.append(x)\n",
    "                table_timestamps.append(t)\n",
    "\n",
    "        decreasing = (positions[-1] - positions[0]) < 0\n",
    "        ordered_positions, ordered_timestamps = [], []\n",
    "\n",
    "        if decreasing:\n",
    "            for i in range(len(positions)-1):\n",
    "                if positions[i + 1] < positions[i]:\n",
    "                    ordered_positions.append(positions[i])\n",
    "                    ordered_timestamps.append(table_timestamps[i])\n",
    "        else:\n",
    "            for i in range(len(positions)-1):\n",
    "                if positions[i + 1] > positions[i]:\n",
    "                    ordered_positions.append(positions[i])\n",
    "                    ordered_timestamps.append(table_timestamps[i])\n",
    "\n",
    "        ordered_positions = np.array(ordered_positions[::-1])\n",
    "        ordered_timestamps = np.array(ordered_timestamps[::-1])\n",
    "\n",
    "        min_t = np.nanmin(ordered_timestamps)\n",
    "\n",
    "        with open(self.lidar_dbg_path) as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "            lidar_timestamps = []\n",
    "            char = '-'\n",
    "            for line in lines:\n",
    "                if len(line) > 40:\n",
    "                    time = line.split('T')[1]\n",
    "                    parts = time.split(':')\n",
    "                    t = float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2].split(char)[0]) \n",
    "\n",
    "                    lidar_timestamps.append(t - min_t)\n",
    "\n",
    "        lidar_timestamps = np.array(lidar_timestamps)\n",
    "        ordered_timestamps -= min_t\n",
    "\n",
    "        ordered_timestamps = np.concatenate([ordered_timestamps[:2], ordered_timestamps[-2:]])\n",
    "        ordered_positions = np.concatenate([ordered_positions[:2], ordered_positions[-2:]])\n",
    "\n",
    "        interp_func = interp1d(ordered_timestamps, ordered_positions, kind=\"linear\", fill_value=\"extrapolate\", bounds_error=False) \n",
    "\n",
    "        self.interpolated_x_values = interp_func(lidar_timestamps)\n",
    "\n",
    "    def _load_component_parameters(self):\n",
    "        self.y_offset = 0\n",
    "        if self.component_parameters_path is not None:\n",
    "            with open(self.component_parameters_path) as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    if \"XRAY_DPP[Acquisition]#0.X.Start:\" in line:\n",
    "                        self.x_start = float(line.split(\":\")[1].strip())\n",
    "                    if \"XRAY_DPP[Acquisition]#0.X.Stop:\" in line:\n",
    "                        self.x_stop = float(line.split(\":\")[1].strip())\n",
    "                    if \"XRAY_DPP[Acquisition]#0.Y.Start:\" in line:\n",
    "                        self.y_offset = float(line.split(\":\")[1].strip())\n",
    "    \n",
    "    def _load_lidar_data(self):\n",
    "        if self.lidar2xrf_path is None:\n",
    "            self.transformation_matrix = np.array([[1,0,0,192],\n",
    "                                                  [0,-1,0,9.3],\n",
    "                                                  [0,0,-1,53.8],\n",
    "                                                  [0,0,0,1]])\n",
    "        else: \n",
    "            with open(self.lidar2xrf_path) as file:\n",
    "                lines = file.readlines()\n",
    "                self.transformation_matrix = np.array([list(map(float, line.strip().split(\",\"))) for line in lines])\n",
    "\n",
    "        self.point_cloud = np.fromfile(self.bpc_path, dtype=np.float32).reshape(-1, 3)\n",
    "        intensity_values = cv2.imread(self.intensity_path, cv2.IMREAD_ANYDEPTH)\n",
    "        intensity_values = np.reshape(intensity_values,(-1,1))\n",
    "\n",
    "        self.original_cloud_limits = [np.nanmax(self.point_cloud[:,0]),np.nanmin(self.point_cloud[:,0]), len(np.unique(self.point_cloud[:,0])),\n",
    "                                      np.nanmax(self.point_cloud[:,1]),np.nanmin(self.point_cloud[:,1]), len(np.unique(self.point_cloud[:,1])),\n",
    "                                      np.nanmax(self.point_cloud[:,2]),np.nanmin(self.point_cloud[:,2]), len(self.point_cloud[:,2])]\n",
    "\n",
    "        self.intensity_cloud = np.hstack((self.point_cloud[:,:2], intensity_values))\n",
    "        self.point_cloud = (np.hstack((self.point_cloud, np.ones((self.point_cloud.shape[0], 1)))) @ self.transformation_matrix.T)[:,:3]\n",
    "        self.intensity_cloud = (np.hstack((self.intensity_cloud, np.ones((self.intensity_cloud.shape[0], 1)))) @ self.transformation_matrix.T)[:,:3]\n",
    "\n",
    "        self.translated_cloud_limits = [np.nanmax(self.point_cloud[:,0]),np.nanmin(self.point_cloud[:,0]), len(np.unique(self.point_cloud[:,0])),\n",
    "                                      np.nanmax(self.point_cloud[:,1]),np.nanmin(self.point_cloud[:,1]), len(np.unique(self.point_cloud[:,1])),\n",
    "                                      np.nanmax(self.point_cloud[:,2]),np.nanmin(self.point_cloud[:,2]), len(self.point_cloud[:,2])]\n",
    "    \n",
    "    def _build_processor(self):\n",
    "        self._find_files()\n",
    "        self._validate_files()\n",
    "        self._load_component_parameters()\n",
    "        self._load_lidar_data()\n",
    "        self._interpolate_x_positions()\n",
    "        \n",
    "        mask = (self.point_cloud[:,0] <= self.x_start) & (self.point_cloud[:,0] >= self.x_stop) & ((self.point_cloud[:,1]) <= (self.window_size + 5 + self.y_shift)) & ((self.point_cloud[:,1]) >=  -(self.window_size + 5 - self.y_shift))\n",
    "        self.point_cloud = self.point_cloud[mask]\n",
    "        self.intensity_cloud = self.intensity_cloud[mask]\n",
    "        self.point_cloud[:,1] -= self.y_shift\n",
    "        self.intensity_cloud[:,1] -= self.y_shift\n",
    "\n",
    "        min_intensity = 0\n",
    "        min_z = 250\n",
    "        \n",
    "        x_values = np.unique(self.point_cloud[:,0])\n",
    "        y_values = np.unique(self.point_cloud[:,1]) \n",
    "        \n",
    "        x_range = len(x_values)\n",
    "\n",
    "        if self.upsample_ratio > 1:\n",
    "            index_step = (np.nanmedian(np.diff(x_values))) / self.upsample_ratio \n",
    "            index_steps = np.arange(int(round(np.nanmin(x_values)))/index_step - 1,int(round((np.nanmax(x_values))/index_step)+1)) * index_step\n",
    "            x_range = len(index_steps)\n",
    "            known_indices = np.unique([np.argmin(np.abs(index_steps - x)) for x in x_values])\n",
    "            unique_indices, unique_idx = np.unique(known_indices, return_index=True)\n",
    "            known_indices = unique_indices\n",
    "            known_x_values = x_values[unique_idx]\n",
    "            all_indices = np.arange(x_range)\n",
    "            interp_func = interp1d(known_indices, known_x_values, kind=\"linear\", fill_value=\"extrapolate\", bounds_error=False)\n",
    "            x_values = interp_func(all_indices)\n",
    "        else: self.upsample_ratio = 1\n",
    "        \n",
    "        x_value_dict = {x: index for index,x in enumerate(x_values)}\n",
    "        y_value_dict = {y: index for index,y in enumerate(y_values)}\n",
    "\n",
    "        point_array = np.full((len(y_values), x_range),np.nan)\n",
    "        intensity_array = np.full((len(y_values), x_range),np.nan)\n",
    "        point_dictionary = {(row[0],row[1]): (index, row[2]) for index,row in enumerate(self.point_cloud)}\n",
    "        intensity_dictionary = {(row[0],row[1]): (index, row[2]) for index,row in enumerate(self.intensity_cloud)}\n",
    "\n",
    "        for x in range (x_range):\n",
    "            for y in range (len(y_values)):\n",
    "                x_val = x_values[x]\n",
    "                y_val = y_values[y]\n",
    "\n",
    "                point_z = point_dictionary.get((x_val,y_val))\n",
    "                intensity_z = intensity_dictionary.get((x_val,y_val))\n",
    "\n",
    "                if point_z is not None:\n",
    "                    point_array[y,x] = point_z[1]\n",
    "                else:\n",
    "                    point_array[y,x] = min_z\n",
    "                if intensity_z is not None:\n",
    "                    intensity_array[y,x] = intensity_z[1]\n",
    "                else:\n",
    "                    intensity_array[y,x] = min_intensity\n",
    "\n",
    "        if self.upsample_ratio > 1:\n",
    "            for y in range(len(y_values)):\n",
    "\n",
    "                interpolated_z_values= []\n",
    "                interpolated_i_values = []\n",
    "\n",
    "                for dy in [-4,-3,-2,-1,0,1,2,3,4]:\n",
    "                    y_dy = y + dy\n",
    "                    if (y_dy >= 0) & (y_dy < len(y_values)):\n",
    "                        known_z = point_array[y_dy,known_indices]\n",
    "                        known_i = intensity_array[y_dy,known_indices]\n",
    "                        known_x = x_values[known_indices]\n",
    "                        mask = ~np.isnan(known_z)\n",
    "                        z_interp = interp1d(known_x[mask], known_z[mask], kind='linear', fill_value=\"extrapolate\")\n",
    "                        i_interp = interp1d(known_x[mask], known_i[mask], kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "                        interpolated_z_values.append(z_interp(x_values))\n",
    "                        interpolated_i_values.append(i_interp(x_values))\n",
    "\n",
    "                point_array[y, :] = np.mean(interpolated_z_values, axis=0)\n",
    "                intensity_array[y, :] = np.mean(interpolated_i_values, axis=0)\n",
    "\n",
    "        self.y_seed_point = np.argmin(np.abs(y_values))\n",
    "        self.point_cloud = point_array\n",
    "        self.intensity_cloud = intensity_array\n",
    "        self.x_to_i_dict = x_value_dict\n",
    "        self.i_to_x_list = x_values\n",
    "        self.y_to_i_dict = y_value_dict\n",
    "        self.i_to_y_list = y_values\n",
    "        self.y_pixel_size = np.nanmedian(np.diff(y_values))\n",
    "        self.x_pixel_size = np.nanmedian(np.diff(x_values))\n",
    "    \n",
    "    def _create_PCA_mask(self, x_window = 9, y_window = 2, batch_size = 100, **kwargs):\n",
    "\n",
    "        y_window = int(cp.round(0.75/self.y_pixel_size))\n",
    "        x_window = int(cp.round(1.5/self.x_pixel_size))\n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        s = self.strong_PCA_threshold \n",
    "        w = self.weak_PCA_threshold \n",
    "        pc = cp.array(self.point_cloud)\n",
    "        y_values = self.i_to_y_list\n",
    "\n",
    "        y_range, x_range = 2 * y_window + 1, 2 * x_window + 1\n",
    "\n",
    "        y_bottom = np.argmin(np.abs(y_values - self.window_size))\n",
    "        y_top = np.argmin(np.abs(y_values + self.window_size))\n",
    "\n",
    "        x_n = cp.arange(-x_window, x_window + 1) * self.x_pixel_size\n",
    "        y_n = cp.arange(-y_window, y_window + 1) * self.y_pixel_size\n",
    "\n",
    "        valid_x = cp.repeat(x_n, y_range).flatten()\n",
    "        valid_y = cp.tile(y_n, x_range).flatten()\n",
    "        eigvals = cp.zeros_like(pc) \n",
    "        pc = cp.pad(pc, ((y_window,y_window), (x_window,x_window)), mode='edge')\n",
    "\n",
    "        for x in range(x_window, pc.shape[1] - x_window, batch_size):\n",
    "            for y in range(y_window, pc.shape[0] - y_window, batch_size):\n",
    "                batch = pc[y - y_window:y + batch_size + y_window, x - x_window:x + batch_size + x_window]\n",
    "\n",
    "                points = cp.lib.stride_tricks.sliding_window_view(batch, (y_range, x_range))\n",
    "\n",
    "                height, width = points.shape[:2]\n",
    "                points = points.reshape(height, width, y_range * x_range)\n",
    "\n",
    "                points = cp.stack((\n",
    "                    cp.broadcast_to(valid_x, (height, width, y_range * x_range)),\n",
    "                    cp.broadcast_to(valid_y, (height, width, y_range * x_range)),\n",
    "                    points), axis=2)\n",
    "\n",
    "                mean_vals = cp.mean(points[:, :, 2, :], axis=2)\n",
    "                points[:, :, 2, :] -= mean_vals[..., None]\n",
    "\n",
    "                cov_matrices = cp.einsum('...ik,...jk->...ij', points, points) / (y_range * x_range - 1)\n",
    "                evals = cp.linalg.eigvalsh(cov_matrices)\n",
    "                e2 = evals[:, :, 1] \n",
    "\n",
    "                eigvals[y - y_window:y + batch_size - y_window, x - x_window:x + batch_size - x_window] = e2 \n",
    "\n",
    "        eigvals = cp.asnumpy(eigvals)\n",
    "\n",
    "        eigvals[y_bottom:,:] = 0\n",
    "        eigvals[:y_top,:] = 0\n",
    "        neighbours = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "        strong_y, strong_x = np.where(eigvals >= s)\n",
    "        weak_values = (eigvals >= w) & (eigvals < s).astype(np.uint8) \n",
    "        return_array = np.zeros_like(eigvals, dtype=cp.uint8)\n",
    "        \n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "        queue = deque(zip(strong_y, strong_x))\n",
    "        while queue:\n",
    "            y, x = queue.popleft() \n",
    "            return_array[y, x] = 1\n",
    "            for dy, dx in neighbours:\n",
    "                ny, nx = y + dy, x + dx\n",
    "                if 0 <= ny < weak_values.shape[0] and 0 <= nx < weak_values.shape[1]:\n",
    "                    if weak_values[ny, nx]:\n",
    "                        weak_values[ny, nx] = 0 \n",
    "                        queue.append((ny, nx)) \n",
    "        del weak_values \n",
    "\n",
    "        self.PCA_mask = return_array\n",
    "\n",
    "    def _get_gradient(self):\n",
    "        array_i = cp.array(np.log(np.abs(self.intensity_cloud)), dtype=cp.float32)\n",
    "        array_z = cp.array(self.point_cloud, dtype=cp.float32)\n",
    "        y_values = self.i_to_y_list\n",
    "        sobel_y = cp.array([\n",
    "                [-20.75,],\n",
    "                [ -11.6,],\n",
    "                [ -6.27,],\n",
    "                [    -2,],\n",
    "                [     0,],\n",
    "                [     2,],\n",
    "                [  6.27,],\n",
    "                [  11.6,],\n",
    "                [ 20.75,]\n",
    "        ])\n",
    "        sobel_x = sobel_y.T\n",
    "\n",
    "        y_grad_i = cp.abs(cpconvolve(array_i, sobel_y))\n",
    "        x_grad_i = cp.abs(cpconvolve(array_i, sobel_x))\n",
    "        magnitude_i = cp.sqrt(x_grad_i**2 + y_grad_i**2)\n",
    "        y_grad_i = cp.abs(cpconvolve(array_i, sobel_y))\n",
    "        magnitude_i = cp.sqrt(x_grad_i**2 + y_grad_i**2)\n",
    "\n",
    "        x_grad_z = cp.abs(cpconvolve(array_z, sobel_y))\n",
    "        y_grad_z = cp.abs(cpconvolve(array_z, sobel_x))\n",
    "        magnitude_z = cp.sqrt(x_grad_z**2 + y_grad_z**2)\n",
    "        y_grad_z = cp.abs(cpconvolve(magnitude_z, sobel_x))\n",
    "        magnitude_z = cp.sqrt(x_grad_z**2 + y_grad_z**2)\n",
    "\n",
    "        magnitude = cp.sqrt(magnitude_i * magnitude_z)\n",
    "\n",
    "        y_bottom = np.argmin(np.abs(y_values - self.window_size))\n",
    "        y_top = np.argmin(np.abs(y_values + self.window_size))\n",
    "\n",
    "        magnitude[:y_top, :] = 0\n",
    "        magnitude[y_bottom:, :] = 0\n",
    "\n",
    "        self.gradient = cp.asnumpy(magnitude)\n",
    "    \n",
    "    def _create_edge_array(self,  **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        self._get_gradient()\n",
    "\n",
    "        self._create_PCA_mask()\n",
    "\n",
    "\n",
    "        y_values = self.i_to_y_list\n",
    "        neighbours = [(-1, 0), (0, -1), (0, 1), (1, 0)]\n",
    "    \n",
    "        result_array = np.zeros_like(self.gradient)\n",
    "\n",
    "        masked_gradient = self.gradient * self.PCA_mask \n",
    "        \n",
    "        strong_y, strong_x = np.where(masked_gradient >= self.strong_edge_threshold)\n",
    "        weak_edges = ((self.gradient >= self.weak_edge_threshold) & (masked_gradient < self.strong_edge_threshold)).astype(np.uint8) \n",
    "\n",
    "        queue = deque(zip(strong_y, strong_x))\n",
    "        while queue:\n",
    "            y, x = queue.popleft() \n",
    "            result_array[y, x] = 1\n",
    "            for dy, dx in neighbours:\n",
    "                ny, nx = y + dy, x + dx\n",
    "                if 0 <= ny < self.gradient.shape[0] and 0 <= nx < self.gradient.shape[1]:\n",
    "                    if weak_edges[ny, nx]:\n",
    "                        weak_edges[ny, nx] = 0 \n",
    "                        queue.append((ny, nx)) \n",
    "\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n",
    "        y_top = np.argmin(np.abs(y_values - self.window_size))\n",
    "        y_bottom = np.argmin(np.abs(y_values + self.window_size))\n",
    "        \n",
    "\n",
    "        result_array = cv2.morphologyEx(result_array, cv2.MORPH_CLOSE, kernel)\n",
    "        result_array[y_top, :] = 1\n",
    "        result_array[y_bottom, :] = 1\n",
    "\n",
    "        self.edge_array = np.where(result_array ==  1, 1 , np.nan)\n",
    "\n",
    "    def _get_props(self, distribution1, distribution2):\n",
    "            \n",
    "            properties = []\n",
    "            return_properties = []\n",
    "\n",
    "            variance =  np.var(distribution1) *((10/self.window_size)**2)\n",
    "            skw = skew(distribution1)\n",
    "            kurt = kurtosis(distribution1)\n",
    "            mean = np.sqrt(np.mean(np.abs(distribution2)))\n",
    "            max = np.sqrt(np.max(np.abs(distribution2)))\n",
    "\n",
    "            \n",
    "            properties.append(variance)\n",
    "            properties.append(skw)\n",
    "            properties.append(kurt)\n",
    "\n",
    "            norm = np.linalg.norm(properties)\n",
    "            \n",
    "            if norm > 0:\n",
    "                properties = [x / norm for x in properties]\n",
    "\n",
    "            z = properties[0]\n",
    "\n",
    "            return_properties.append(mean)\n",
    "            return_properties.append(max)\n",
    "            return_properties.append(np.abs(properties[1]))\n",
    "            return_properties.append(np.arccos(z)/(np.pi/2))\n",
    "\n",
    "            return return_properties, np.uint8(properties[1] > 0)\n",
    "        \n",
    "    def _load_DBSCAN_model(self):\n",
    "                with open(self.DBSCAN_model_path, 'rb') as f:\n",
    "                    self.tree, self.labels = pickle.load(f)\n",
    "\n",
    "    def _define_sections(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        self._load_DBSCAN_model()\n",
    "        self._create_edge_array()\n",
    "\n",
    "        kernel = np.array([[-1], [-2], [0], [2], [1]])\n",
    "        convolved_point_cloud = convolve(self.point_cloud, kernel)\n",
    "\n",
    "        nan_mask = np.isnan(self.edge_array)\n",
    "        structure = np.array([[0,1,0],\n",
    "                            [1,1,1],\n",
    "                            [0,1,0]])  \n",
    "        labeled_mask, num_regions = label(nan_mask, structure=structure)\n",
    "\n",
    "        self.labeled_x_values = np.zeros(self.point_cloud.shape[1], dtype=int)\n",
    "\n",
    "        for region_id in range(1, num_regions + 1):\n",
    "            region_mask = labeled_mask == region_id\n",
    "            ys, xs = np.where(region_mask)\n",
    "\n",
    "            if not np.any(ys == self.y_seed_point):\n",
    "                continue\n",
    "            \n",
    "            center_x_vals = xs[ys == self.y_seed_point]\n",
    "\n",
    "            vectors = []\n",
    "            skew = 0\n",
    "\n",
    "            for cx in center_x_vals:\n",
    "                mask = xs == cx\n",
    "                ys_at_x = ys[mask]\n",
    "\n",
    "                if ys_at_x.size == 0:\n",
    "                    continue\n",
    "\n",
    "                z = self.point_cloud[ys_at_x, cx]\n",
    "                cz = convolved_point_cloud[ys_at_x, cx]\n",
    "\n",
    "                v = self._get_props(z, cz)\n",
    "                vectors.append(v[0])\n",
    "                skew += 1 if v[1] else -1\n",
    "\n",
    "            if not vectors:\n",
    "                continue\n",
    "\n",
    "            v = np.mean(vectors, axis=0, keepdims=True)\n",
    "            b = skew > 0\n",
    "            closest_label = 0\n",
    "\n",
    "            if not np.any(np.isnan(v)):\n",
    "                dist, ind = self.tree.query([v], k=1)\n",
    "                if dist[0] <= self.noise_threshold:\n",
    "                    closest_label = self.labels[ind[0]]\n",
    "                    if closest_label == 2:\n",
    "                        closest_label += b\n",
    "\n",
    "            for cx in center_x_vals:\n",
    "                self.labeled_x_values[cx] = closest_label\n",
    "\n",
    "    def _classify_rubble(self, x_window= 5, y_window=4, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        pc = cp.array(self.point_cloud)\n",
    "        pc = pc[self.y_seed_point - y_window:self.y_seed_point + y_window + 1,:]\n",
    "\n",
    "        width = pc.shape[1]\n",
    "        y_range, x_range = 2 * y_window + 1, 2 * x_window + 1\n",
    "\n",
    "        x_n = cp.arange(-x_window, x_window + 1) * self.x_pixel_size\n",
    "        y_n = cp.arange(-y_window, y_window + 1) * self.y_pixel_size\n",
    "        valid_x = cp.repeat(x_n, y_range).flatten()\n",
    "        valid_y = cp.tile(y_n, x_range).flatten()\n",
    "\n",
    "        points = cp.lib.stride_tricks.sliding_window_view(\n",
    "            cp.pad(pc, ((0,0), (x_window,x_window)), mode='edge'), (y_range, x_range)\n",
    "        )\n",
    "\n",
    "        points = points.reshape(width, y_range*x_range)\n",
    "\n",
    "        points = cp.stack((\n",
    "            cp.broadcast_to(valid_x, (width, y_range*x_range)),\n",
    "            cp.broadcast_to(valid_y, (width, y_range*x_range)),\n",
    "            points), axis=2)\n",
    "        mean_vals = cp.mean(points[:,:,2], axis=(1), keepdims=True)\n",
    "        points[:,:,2] -= mean_vals\n",
    "\n",
    "        cov_matrices = cp.matmul(points.transpose(0,2,1), points) / (y_range * x_range - 1)\n",
    "\n",
    "        eigvals, eigvecs = cp.linalg.eigh(cov_matrices)\n",
    "\n",
    "        eigvecs = eigvecs[:, :, 0]\n",
    "\n",
    "        eigvals = eigvals[:,0]/(eigvals[:,0] + eigvals[:,1]  + eigvals[:,2]) \n",
    "        \n",
    "        x_offset = cp.abs(cp.arctan(eigvecs[:,0]/eigvecs[:,2]))\n",
    "        y_offset = cp.abs(cp.arctan(eigvecs[:,1]/eigvecs[:,2]))\n",
    "\n",
    "        results = {self.i_to_x_list[i]: [self.labeled_x_values[i],float(eigvals[i]),float(x_offset[i]),float(y_offset[i])] for i in range(len(self.i_to_x_list))}\n",
    "\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    " \n",
    "        self.rubble_classifications = results\n",
    "   \n",
    "    def _define_correction_windows(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        self._define_sections()\n",
    "        self._classify_rubble()\n",
    "\n",
    "        windows = {}\n",
    "\n",
    "        for x_start in np.arange(0, np.nanmax(self.i_to_x_list), self.xrf_window_size):\n",
    "            values = [self.rubble_classifications[i] for i, x in zip(self.i_to_x_list, self.i_to_x_list) \n",
    "            if x_start <= x < x_start + self.xrf_window_size]\n",
    "            if values:\n",
    "                values = np.array(values)\n",
    "        \n",
    "                x_end = max(x for x in self.i_to_x_list if x_start <= x < x_start + self.xrf_window_size)\n",
    "                x_start = min(x for x in self.i_to_x_list if x_start <= x < x_start + self.xrf_window_size)\n",
    "            \n",
    "                \n",
    "                half_perc = np.sum(values[:,0] == 1) / len(values)\n",
    "                empty_perc = np.sum(values[:,0] == 2) / len(values)\n",
    "                full_perc = np.sum(values[:,0] == 3) / len(values)\n",
    "\n",
    "                rubble_perc = 1 - half_perc - empty_perc - full_perc\n",
    "\n",
    "                avg_var = np.nanmean(values[:, 1])\n",
    "                avg_x_offset = np.nanmean(values[:, 2])\n",
    "                avg_y_offset = np.nanmean(values[:, 3])\n",
    "\n",
    "                windows[(x_start, x_end)] = [half_perc, full_perc, empty_perc, rubble_perc, avg_var, avg_x_offset, avg_y_offset]\n",
    "\n",
    "        self.correction_windows = windows\n",
    "\n",
    "    def _save_correction_windows(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "        \n",
    "        self._define_correction_windows()\n",
    "        \n",
    "        os.makedirs(self.file_path, exist_ok=True)\n",
    "        \n",
    "        file_path = os.path.join(self.file_path, 'rubble_classification.pkl')\n",
    "        \n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(self.correction_windows, f)\n",
    "\n",
    "    def _plot_correction_windows(self, width = 150, height = 7, dpi = 75, plot_point_cloud = False, **kwargs):\n",
    "        \n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        self._define_correction_windows()\n",
    "\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "\n",
    "        colormap = [\"green\", \"blue\", \"purple\", \"red\"]\n",
    "        cmap1 = plt.cm.cool  \n",
    "        cmap2 = plt.cm.cool\n",
    "\n",
    "        all_vars = []\n",
    "        x_angles = []\n",
    "        y_angles = []\n",
    "\n",
    "        for values in self.correction_windows.values():\n",
    "            var, x_angle, y_angle = values[4:7]\n",
    "            all_vars.append(var)\n",
    "            x_angles.append(x_angle)\n",
    "            y_angles.append(y_angle)\n",
    "\n",
    "        min_var = np.nanmin(all_vars)\n",
    "        max_var = np.nanmax(all_vars)\n",
    "        min_x_angle = np.nanmin(x_angles)\n",
    "        max_x_angle = np.nanmax(x_angles)\n",
    "        min_y_angle = np.nanmin(y_angles)\n",
    "        max_y_angle = np.nanmax(y_angles)\n",
    "\n",
    "        if plot_point_cloud:\n",
    "            display = self.point_cloud\n",
    "        else:\n",
    "            display = self.intensity_cloud\n",
    "        ax.imshow(cp.flipud(display), cmap=self.colourmap, interpolation='nearest', alpha=1, aspect= 'auto')\n",
    "\n",
    "        for (x_start, x_end), values in self.correction_windows.items():\n",
    "            half_perc, empty_perc, full_perc, rubble_perc = values[:4]\n",
    "\n",
    "            ratios = np.array([half_perc, empty_perc, full_perc, rubble_perc])\n",
    "            ratios /= ratios.sum()  \n",
    "\n",
    "            start_i = int(np.where(self.i_to_x_list == x_start)[0][0])\n",
    "            end_i = int(np.where(self.i_to_x_list == x_end)[0][0])\n",
    "            width = end_i - start_i + 1\n",
    "            bar_heights = ratios * 12  \n",
    "            bottoms = self.y_seed_point - 6 + np.insert(np.cumsum(bar_heights[:-1]), 0, 0)\n",
    "\n",
    "            for j in range(4):\n",
    "                ax.bar(start_i, height=bar_heights[j], width=width,\n",
    "                    bottom=bottoms[j], color=colormap[j], alpha=1, align = 'edge')\n",
    "\n",
    "            var, x_angle, y_angle = values[4:]\n",
    "            norm1 = plt.Normalize(vmin=min_var, vmax=max_var)\n",
    "            norm2 = plt.Normalize(vmin=min_x_angle, vmax=max_x_angle)\n",
    "            norm3 = plt.Normalize(vmin=min_y_angle, vmax=max_y_angle)\n",
    "            color1 = cmap1(norm1(var))\n",
    "            color2 = cmap2(norm2(x_angle))\n",
    "            color3 = cmap2(norm3(y_angle))\n",
    "            extra_heights = 7.5\n",
    "\n",
    "            ax.bar(start_i, height=extra_heights, width=width,\n",
    "                bottom=0, color=color1, align = 'edge')\n",
    "            ax.bar(start_i, height=extra_heights, width=width,\n",
    "                bottom=extra_heights, color=color2, align = 'edge')\n",
    "            ax.bar(start_i, height=extra_heights, width=width,\n",
    "                bottom=extra_heights*2, color=color3, align = 'edge')\n",
    "\n",
    "        ax.set_xlabel(\"X index\",fontsize = 20)\n",
    "        ax.set_ylabel(\"Y index\",fontsize = 20)\n",
    "        ax.set_xlim(0, display.shape[1])\n",
    "        ax.set_title(self.name, fontsize=35)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_gradient(self,width = 150, height = 7, dpi = 75, **kwargs):\n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "        \n",
    "        self._get_gradient()\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "        ax.imshow(cp.flipud(self.gradient), cmap='nipy_spectral_r', interpolation='nearest', alpha = 1, aspect = 'auto')\n",
    "        ax.set_xlabel(\"X index\",fontsize = 20)\n",
    "        ax.set_ylabel(\"Y index\",fontsize = 20)\n",
    "        ax.set_title(self.name, fontsize=35)\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_edges(self,width = 150, height = 7, dpi = 75, **kwargs):\n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        self._create_edge_array()\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "        ax.imshow(cp.flipud(self.edge_array), cmap='nipy_spectral', interpolation='nearest', alpha = 1, aspect = 'auto')\n",
    "        ax.set_xlabel(\"X index\",fontsize = 20)\n",
    "        ax.set_ylabel(\"Y index\",fontsize = 20)\n",
    "        ax.set_title(self.name, fontsize=35)\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_PCA_mask(self, width = 150, height = 7, dpi = 75, **kwargs):\n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "\n",
    "\n",
    "        self._create_PCA_mask()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "        a = 1\n",
    "\n",
    "        ax.imshow(cp.flipud(self.PCA_mask), cmap='nipy_spectral', interpolation='nearest', alpha = a, aspect = 'auto')\n",
    "        ax.set_xlabel(\"X index\")\n",
    "        ax.set_ylabel(\"Y index\")\n",
    "        ax.set_title(self.name)\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_sections(self, plot_point_cloud = False, plot_intensity_cloud = True, width = 150, height = 7, dpi = 75, **kwargs):\n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        self._create_edge_array()\n",
    "        \n",
    "        sections = self.edge_array.copy()\n",
    "    \n",
    "        neighbours = [(-1, 0), (0, -1), (0, 1), (1, 0)]  \n",
    "        nan_mask = np.isnan(sections)  \n",
    "\n",
    "        int = 2\n",
    "        for x in range(sections.shape[1]):\n",
    "            if nan_mask[self.y_seed_point, x]: \n",
    "                queue = deque([(self.y_seed_point, x)])\n",
    "                while queue:\n",
    "                    y, x = queue.popleft()\n",
    "                    sections[y, x] = int \n",
    "                    nan_mask[y, x] = False  \n",
    "                    neighbors_to_add = [(ny, nx) for dy, dx in neighbours\n",
    "                        if (0 <= (ny := y + dy) < sections.shape[0] and \n",
    "                            0 <= (nx := x + dx) < sections.shape[1] and nan_mask[ny, nx])]\n",
    "                    queue.extend(neighbors_to_add) \n",
    "                    for ny, nx in neighbors_to_add:\n",
    "                        nan_mask[ny, nx] = False \n",
    "            int += 1\n",
    "        \n",
    "        sections = np.where(sections > 1, sections, np.nan)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "        a = 1\n",
    "        if plot_point_cloud:\n",
    "            ax.imshow(cp.flipud(self.point_cloud), cmap=self.colourmap, interpolation='nearest', alpha = 1, aspect= 'auto')\n",
    "            a = 0.5\n",
    "        if plot_intensity_cloud:\n",
    "            ax.imshow(cp.flipud(self.intensity_cloud), cmap=self.colourmap, interpolation='nearest', alpha = 1, aspect= 'auto')\n",
    "            a = 0.5\n",
    "        ax.imshow(cp.flipud(sections), cmap='nipy_spectral', interpolation='nearest', alpha = a, aspect= 'auto')\n",
    "        ax.set_xlabel(\"X index\")\n",
    "        ax.set_ylabel(\"Y index\")\n",
    "        ax.set_title(self.name)\n",
    "        plt.show()\n",
    "\n",
    "    def _view_point_cloud(self, intensity_cloud = False):\n",
    "\n",
    "        x = (list)(self.i_to_x_list) * len(self.i_to_y_list)\n",
    "        y = np.repeat(self.i_to_y_list, len(self.i_to_x_list))\n",
    "\n",
    "        if intensity_cloud:\n",
    "            z  = self.intensity_cloud.flatten()\n",
    "        else:\n",
    "            z = self.point_cloud.flatten()\n",
    "\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        points = np.column_stack((x, y, -z))\n",
    "        pcd.points = o3d.utility.Vector3dVector(points)\n",
    "        \n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    def _plot_point_cloud(self,width = 150, height = 7, dpi = 75):\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "        ax.imshow(cp.flipud(self.point_cloud), cmap=self.colourmap, interpolation='nearest', alpha = 1, aspect = 'auto')\n",
    "        ax.set_xlabel(\"X index\")\n",
    "        ax.set_ylabel(\"Y index\")\n",
    "        ax.set_title(self.name)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_intensity_cloud(self,width = 150, height = 7, dpi = 75):\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "        ax.imshow(cp.flipud(self.intensity_cloud), cmap=self.colourmap, interpolation='nearest', alpha = 1, aspect = 'auto')\n",
    "        ax.set_xlabel(\"X index\")\n",
    "        ax.set_ylabel(\"Y index\")\n",
    "        ax.set_title(self.name)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def _get_data_cube(self):\n",
    "\n",
    "        x = (list)(self.i_to_x_list) * len(self.i_to_y_list)\n",
    "        y = np.repeat(self.i_to_y_list, len(self.i_to_x_list))\n",
    "        z = self.point_cloud.flatten()\n",
    "\n",
    "        return np.column_stack((x, y, z))\n",
    "\n",
    "    def _save_to_las(self):\n",
    "\n",
    "        x = (list)(self.i_to_x_list) * len(self.i_to_y_list)\n",
    "        y = np.repeat(self.i_to_y_list, len(self.i_to_x_list))\n",
    "        z = self.point_cloud.flatten()\n",
    "\n",
    "        file_path = os.path.join(self.file_path, \"point_cloud_trimmed.las\")\n",
    "\n",
    "        header = laspy.LasHeader(point_format=1, version=\"1.2\") \n",
    "        las = laspy.LasData(header)\n",
    "\n",
    "        las.x = np.array(x)\n",
    "        las.y = np.array(y)\n",
    "        las.z = np.array(z)\n",
    "\n",
    "        las.write(file_path)\n",
    "\n",
    "    def _print_scan_limits(self, translated = True):\n",
    "        if translated:\n",
    "            data = self.original_cloud_limits\n",
    "            data[0] = data[0] * self.transformation_matrix[0,0] + self.transformation_matrix[0,3]\n",
    "            data[1] = data[1] * self.transformation_matrix[0,0] + self.transformation_matrix[0,3]\n",
    "            data[3] = data[3] * self.transformation_matrix[1,1] + self.transformation_matrix[1,3]\n",
    "            data[4] = data[4] * self.transformation_matrix[1,1] + self.transformation_matrix[1,3]\n",
    "            data[6] = data[6] * self.transformation_matrix[2,2] + self.transformation_matrix[1,3]\n",
    "            data[7] = data[7] * self.transformation_matrix[2,2] + self.transformation_matrix[2,3]\n",
    "            print(f\"limits in machine coordinates for {self.name}:\")\n",
    "        else:\n",
    "            data = self.original_cloud_limits\n",
    "            print(f\"limits in lidar coordinates for {self.name}:\")\n",
    "\n",
    "        print(f\" x value range: {data[0]}mm to {data[1]}mm, number of lines in the x direction: {data[2]}\")\n",
    "        print(f\" y value range: {data[3]}mm to {data[4]}mm, number of lines in the y direction: {data[5]}\")\n",
    "        print(f\" z value range: {data[6]}mm to {data[7]}mm, total number of points: {data[8]}\")\n",
    "        print(f\"pixel size: {self.x_pixel_size}mm in the x direction, {self.y_pixel_size}mm in the y direction\")\n",
    "\n",
    "    def _get_moment_space(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        vectors = []\n",
    "        \n",
    "\n",
    "        y_bottom = np.argmin(np.abs(self.i_to_y_list + self.window_size))\n",
    "        y_top = np.argmin(np.abs(self.i_to_y_list - self.window_size))\n",
    "        \n",
    "        y_bottom, y_top = sorted((y_bottom, y_top))\n",
    "        \n",
    "        convolved_point_cloud = convolve(self.point_cloud,np.array([[-1],[-2],[0],[2],[1]])) \n",
    "        \n",
    "        for i in range(self.point_cloud.shape[1]):\n",
    "\n",
    "            distribution = self.point_cloud[y_bottom:y_top,i]\n",
    "\n",
    "            convolved_distribution = convolved_point_cloud[y_bottom:y_top,i]\n",
    "\n",
    "            vector = self._get_props(distribution,convolved_distribution)\n",
    "\n",
    "            if vector[1] == 0:\n",
    "                vector[0][2] *= -1\n",
    "\n",
    "            if not np.any(np.isnan(vector[0])):\n",
    "                vectors.append(vector[0])\n",
    "        \n",
    "\n",
    "        return vectors\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = r\"C:\\Users\\eashenhurst\\Desktop\\local scans\\test_csv_core\"\n",
    "filter = \"\"\n",
    "\n",
    "paths_list = []\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "        for dir_name in dirs:\n",
    "            if dir_name.startswith(\"Core\"):\n",
    "                folder = os.path.join(root,dir_name)\n",
    "                if filter in folder:\n",
    "                    paths_list.append(folder)\n",
    "\n",
    "processors = []\n",
    "for folder in paths_list:\n",
    "        try:\n",
    "            processor = lidar_processor(folder, name = folder.split('\\\\')[-1], window_size= 17.5, x_stop = 850, box_length = 650)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"   Error: {e}\")\n",
    "        else:  \n",
    "            processors.append(processor)\n",
    "            print(processor.name)\n",
    "            processor._plot_intensity_cloud()\n",
    "\n",
    "print(f\"{len(processors)} Lidar Processors created\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for LP in processors:\n",
    "    LP._plot_sections(strong_edge_threshold = 250, weak_edge_threshold = 75, noise_threshold = 0.1)\n",
    "    LP._plot_correction_windows(strong_edge_threshold = 250, weak_edge_threshold = 75, noise_threshold = 0.1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processors[0]._save_to_las()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LP_env",
   "language": "python",
   "name": "lp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
