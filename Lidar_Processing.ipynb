{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import laspy\n",
    "import matplotlib.pyplot    as plt\n",
    "import numpy                as np\n",
    "import cupy                 as cp\n",
    "import open3d               as o3d\n",
    "from scipy.stats            import skew, kurtosis\n",
    "from scipy.ndimage          import convolve\n",
    "from cupyx.scipy.ndimage    import convolve as cpconvolve\n",
    "from collections            import deque\n",
    "from scipy.interpolate      import interp1d\n",
    "from scipy.spatial import KDTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lidarProcessor:\n",
    "    def __init__(self, file_path, DBSCAN_model_path = None, window_size = 10, upsample_ratio = 2, y_shift = 0, name=None,  \n",
    "                strong_edge_threshold = 80, weak_edge_threshold = 30, strong_PCA_threshold = 0.01, weak_PCA_threshold = 0.0075, colourmap = 'binary'):\n",
    "        if DBSCAN_model_path is None:\n",
    "            current_dir = os.getcwd()\n",
    "            self.DBSCAN_model_path = os.path.join(current_dir, 'cluster_kd_tree.pkl')\n",
    "        else: \n",
    "            self.DBSCAN_model_path = DBSCAN_model_path\n",
    "            \n",
    "        self.file_path = file_path\n",
    "        self.colourmap = colourmap\n",
    "        self.name = name if name else file_path\n",
    "        self.upsample_ratio = upsample_ratio\n",
    "        self.window_size = window_size\n",
    "        self.strong_edge_threshold = strong_edge_threshold\n",
    "        self.weak_edge_threshold = weak_edge_threshold\n",
    "        self.strong_PCA_threshold = strong_PCA_threshold\n",
    "        self.weak_PCA_threshold = weak_PCA_threshold\n",
    "        self.y_shift = y_shift\n",
    "        self.tree = None\n",
    "        self.labels = None\n",
    "\n",
    "        self.correction_windows = None\n",
    "        self.labeled_x_values = None   \n",
    "\n",
    "        self.edge_array = None\n",
    "        self.gradient = None\n",
    "        self.PCA_mask = None\n",
    "        self.x_start = None\n",
    "        self.x_stop = None\n",
    "        self.y_seed_point = None\n",
    "        self.y_offset = None\n",
    "\n",
    "        \n",
    "\n",
    "        self._build_processor()\n",
    "    \n",
    "    def _collect_garbage(self):\n",
    "        self.tree = None\n",
    "        self.labels = None\n",
    "        self.correction_windows = None\n",
    "        self.labeled_x_values = None\n",
    "        self.gradient = None\n",
    "        self.PCA_mask = None\n",
    "        self.component_parameters_path = None\n",
    "        self.lidar2xrf_path = None\n",
    "        self.bpc_path = None\n",
    "        self.intensity_path = None\n",
    "        self.real_dbg_path = None\n",
    "        self.lidar_dbg_path = None\n",
    "\n",
    "    def _find_files(self):\n",
    "        self.component_parameters_path = None\n",
    "        self.lidar2xrf_path = None\n",
    "        self.bpc_path = None\n",
    "        self.intensity_path = None\n",
    "        self.real_dbg_path = None\n",
    "        self.lidar_dbg_path = None\n",
    "        for file_name in os.listdir(self.file_path):\n",
    "            full_path = os.path.join(self.file_path, file_name)\n",
    "            if file_name.endswith(\".component_parameters.txt\"):\n",
    "                self.component_parameters_path = full_path\n",
    "            elif file_name.endswith(\".lidar2xrf\"):\n",
    "                self.lidar2xrf_path = full_path\n",
    "            elif file_name.endswith(\".bpc\"):\n",
    "                self.bpc_path = full_path\n",
    "            elif file_name.endswith(\"_intensity.png\"):\n",
    "                self.intensity_path = full_path\n",
    "            elif file_name.endswith(\"real.dbg\"):\n",
    "                self.real_dbg_path = full_path\n",
    "            elif file_name.endswith(\".dbg\"):\n",
    "                self.lidar_dbg_path = full_path\n",
    "\n",
    "    def _validate_files(self):\n",
    "        missing_files = [\n",
    "            name for name, path in {\n",
    "                \"bpc\": self.bpc_path,\n",
    "                \"intensity\": self.intensity_path,\n",
    "                \"real_dbg\": self.real_dbg_path,\n",
    "                \"dbg\": self.lidar_dbg_path,\n",
    "            }.items() if path is None\n",
    "        ]\n",
    "\n",
    "        non_essential_mising_files = [\n",
    "            name for name, path in {\n",
    "                \"component_parameters\": self.component_parameters_path,\n",
    "                \"lidar2xrf\": self.lidar2xrf_path,\n",
    "            }.items() if path is None\n",
    "        ]\n",
    "\n",
    "        if missing_files:\n",
    "            raise FileNotFoundError(f\"Missing required files: {', '.join(missing_files)}\")\n",
    "        if non_essential_mising_files:\n",
    "            print (f\"   Missing: {', '.join(non_essential_mising_files)}, contents will be assumed\")\n",
    "    \n",
    "    def _interpolate_x_positions(self):\n",
    "        with open(self.real_dbg_path) as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        positions = []\n",
    "        table_timestamps = []\n",
    "\n",
    "        char = '-'\n",
    "        for line in lines:\n",
    "            if len(line) > 40:\n",
    "                if \"+\" in line:\n",
    "                    char = '+'\n",
    "                time = line.split('T')[1]\n",
    "                x = float(line.split(',')[0])\n",
    "                parts = time.split(':')\n",
    "                t = float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2].split(char)[0]) \n",
    "                positions.append(x)\n",
    "                table_timestamps.append(t)\n",
    "\n",
    "        decreasing = (positions[-1] - positions[0]) < 0\n",
    "        ordered_positions, ordered_timestamps = [], []\n",
    "\n",
    "        if decreasing:\n",
    "            for i in range(len(positions)-1):\n",
    "                if positions[i + 1] < positions[i]:\n",
    "                    ordered_positions.append(positions[i])\n",
    "                    ordered_timestamps.append(table_timestamps[i])\n",
    "        else:\n",
    "            for i in range(len(positions)-1):\n",
    "                if positions[i + 1] > positions[i]:\n",
    "                    ordered_positions.append(positions[i])\n",
    "                    ordered_timestamps.append(table_timestamps[i])\n",
    "\n",
    "        ordered_positions = np.array(ordered_positions[::-1])\n",
    "        ordered_timestamps = np.array(ordered_timestamps[::-1])\n",
    "\n",
    "        min_t = np.nanmin(ordered_timestamps)\n",
    "\n",
    "        with open(self.lidar_dbg_path) as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "            lidar_timestamps = []\n",
    "            char = '-'\n",
    "            for line in lines:\n",
    "                if len(line) > 40:\n",
    "                    time = line.split('T')[1]\n",
    "                    parts = time.split(':')\n",
    "                    t = float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2].split(char)[0]) \n",
    "\n",
    "                    lidar_timestamps.append(t - min_t)\n",
    "\n",
    "        lidar_timestamps = np.array(lidar_timestamps)\n",
    "        ordered_timestamps -= min_t\n",
    "\n",
    "        ordered_timestamps = np.concatenate([ordered_timestamps[:2], ordered_timestamps[-2:]])\n",
    "        ordered_positions = np.concatenate([ordered_positions[:2], ordered_positions[-2:]])\n",
    "\n",
    "        interp_func = interp1d(ordered_timestamps, ordered_positions, kind=\"linear\", fill_value=\"extrapolate\", bounds_error=False) \n",
    "\n",
    "        self.interpolated_x_values = interp_func(lidar_timestamps)\n",
    "\n",
    "    def _load_component_parameters(self):\n",
    "        self.x_start = 50000\n",
    "        self.x_stop = 0\n",
    "        self.y_offset = 0\n",
    "        if self.component_parameters_path is not None:\n",
    "            with open(self.component_parameters_path) as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    if \"XRAY_DPP[Acquisition]#0.X.Start:\" in line:\n",
    "                        self.x_start = float(line.split(\":\")[1].strip())\n",
    "                    if \"XRAY_DPP[Acquisition]#0.X.Stop:\" in line:\n",
    "                        self.x_stop = float(line.split(\":\")[1].strip())\n",
    "                    if \"XRAY_DPP[Acquisition]#0.Y.Start:\" in line:\n",
    "                        self.y_offset = float(line.split(\":\")[1].strip())\n",
    "    \n",
    "    def _load_lidar_data(self):\n",
    "        if self.lidar2xrf_path is None:\n",
    "            self.transformation_matrix = np.array([[1,0,0,192],\n",
    "                                                  [0,-1,0,9.3],\n",
    "                                                  [0,0,-1,53.8],\n",
    "                                                  [0,0,0,1]])\n",
    "        else: \n",
    "            with open(self.lidar2xrf_path) as file:\n",
    "                lines = file.readlines()\n",
    "                self.transformation_matrix = np.array([list(map(float, line.strip().split(\",\"))) for line in lines])\n",
    "\n",
    "        self.point_cloud = np.fromfile(self.bpc_path, dtype=np.float32).reshape(-1, 3)\n",
    "        intensity_values = cv2.imread(self.intensity_path, cv2.IMREAD_ANYDEPTH)\n",
    "        intensity_values = np.reshape(intensity_values,(-1,1))\n",
    "\n",
    "        self.original_cloud_limits = [np.nanmax(self.point_cloud[:,0]),np.nanmin(self.point_cloud[:,0]), len(np.unique(self.point_cloud[:,0])),\n",
    "                                      np.nanmax(self.point_cloud[:,1]),np.nanmin(self.point_cloud[:,1]), len(np.unique(self.point_cloud[:,1])),\n",
    "                                      np.nanmax(self.point_cloud[:,2]),np.nanmin(self.point_cloud[:,2]), len(self.point_cloud[:,2])]\n",
    "\n",
    "        self.intensity_cloud = np.hstack((self.point_cloud[:,:2], intensity_values))\n",
    "        self.point_cloud = (np.hstack((self.point_cloud, np.ones((self.point_cloud.shape[0], 1)))) @ self.transformation_matrix.T)[:,:3]\n",
    "        self.intensity_cloud = (np.hstack((self.intensity_cloud, np.ones((self.intensity_cloud.shape[0], 1)))) @ self.transformation_matrix.T)[:,:3]\n",
    "\n",
    "        self.translated_cloud_limits = [np.nanmax(self.point_cloud[:,0]),np.nanmin(self.point_cloud[:,0]), len(np.unique(self.point_cloud[:,0])),\n",
    "                                      np.nanmax(self.point_cloud[:,1]),np.nanmin(self.point_cloud[:,1]), len(np.unique(self.point_cloud[:,1])),\n",
    "                                      np.nanmax(self.point_cloud[:,2]),np.nanmin(self.point_cloud[:,2]), len(self.point_cloud[:,2])]\n",
    "    \n",
    "    def _get_gradient(self):\n",
    "        array_i = cp.array(np.log(np.abs(self.intensity_cloud)), dtype=cp.float32)\n",
    "        array_z = cp.array(self.point_cloud, dtype=cp.float32)\n",
    "        y_values = self.i_to_y_list\n",
    "        sobel_y = cp.array([\n",
    "                [-20.75,],\n",
    "                [ -11.6,],\n",
    "                [ -6.27,],\n",
    "                [    -2,],\n",
    "                [     0,],\n",
    "                [     2,],\n",
    "                [  6.27,],\n",
    "                [  11.6,],\n",
    "                [ 20.75,]\n",
    "        ])\n",
    "        sobel_x = sobel_y.T\n",
    "\n",
    "        y_grad_i = cp.abs(cpconvolve(array_i, sobel_y))\n",
    "        x_grad_i = cp.abs(cpconvolve(array_i, sobel_x))\n",
    "        magnitude_i = cp.sqrt(x_grad_i**2 + y_grad_i**2)\n",
    "        y_grad_i = cp.abs(cpconvolve(array_i, sobel_y))\n",
    "        magnitude_i = cp.sqrt(x_grad_i**2 + y_grad_i**2)\n",
    "\n",
    "        x_grad_z = cp.abs(cpconvolve(array_z, sobel_y))\n",
    "        y_grad_z = cp.abs(cpconvolve(array_z, sobel_x))\n",
    "        magnitude_z = cp.sqrt(x_grad_z**2 + y_grad_z**2)\n",
    "        y_grad_z = cp.abs(cpconvolve(magnitude_z, sobel_x))\n",
    "        magnitude_z = cp.sqrt(x_grad_z**2 + y_grad_z**2)\n",
    "\n",
    "        magnitude = cp.sqrt(magnitude_i * magnitude_z)\n",
    "\n",
    "        y_bottom = np.argmin(np.abs(y_values - self.window_size))\n",
    "        y_top = np.argmin(np.abs(y_values + self.window_size))\n",
    "\n",
    "        magnitude[:y_top, :] = 0\n",
    "        magnitude[y_bottom:, :] = 0\n",
    "\n",
    "        self.gradient = cp.asnumpy(magnitude)\n",
    "\n",
    "    def _create_PCA_mask(self,y_window = 9, x_window = 2, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        s = self.strong_PCA_threshold\n",
    "        w = self.weak_PCA_threshold\n",
    "        pc = cp.array(self.point_cloud)\n",
    "        y_values = self.i_to_y_list\n",
    "        height, width = pc.shape\n",
    "\n",
    "        y_range, x_range = 2 * y_window + 1, 2 * x_window + 1\n",
    "\n",
    "        y_bottom = np.argmin(np.abs(y_values - self.window_size))\n",
    "        y_top = np.argmin(np.abs(y_values + self.window_size))\n",
    "\n",
    "        x_n = cp.arange(-x_window, x_window + 1) /self.upsample_ratio\n",
    "        y_n = cp.arange(-y_window, y_window + 1) /4.5\n",
    "        valid_x = cp.repeat(x_n, y_range).flatten()\n",
    "        valid_y = cp.tile(y_n, x_range).flatten()\n",
    "\n",
    "        points = cp.lib.stride_tricks.sliding_window_view(\n",
    "            cp.pad(pc, ((y_window,y_window), (x_window,x_window)), mode='edge'), (y_range, x_range)\n",
    "        )\n",
    "        del pc\n",
    "\n",
    "        points = points.reshape(height,width, y_range*x_range)\n",
    "        \n",
    "        points = cp.stack((\n",
    "            cp.broadcast_to(valid_x, (height,width, y_range*x_range)),\n",
    "            cp.broadcast_to(valid_y, (height,width, y_range*x_range)),\n",
    "            points), axis=2)\n",
    "        mean_vals = cp.mean(points[:,:,2,:], axis=(2), keepdims=True)\n",
    "        points[:,:,2,:] -= mean_vals\n",
    "\n",
    "        cov_matrices = cp.matmul(points, points.transpose(0, 1, 3, 2)) / (points.shape[3] - 1)\n",
    "        del points \n",
    "\n",
    "        a, b, c, d, e, f, g, h, i = (\n",
    "            cov_matrices[:, :, 0, 0], cov_matrices[:, :, 0, 1], cov_matrices[:, :, 0, 2],\n",
    "            cov_matrices[:, :, 1, 0], cov_matrices[:, :, 1, 1], cov_matrices[:, :, 1, 2],\n",
    "            cov_matrices[:, :, 2, 0], cov_matrices[:, :, 2, 1], cov_matrices[:, :, 2, 2]\n",
    "        )\n",
    "\n",
    "        p1 = b**2 + c**2 + f**2\n",
    "        q = (a + e + i) / 3\n",
    "        p2 = (a - q)**2 + (e - q)**2 + (i - q)**2 + 2 * p1\n",
    "        p = cp.sqrt(p2 / 6)\n",
    "        \n",
    "        B = (cov_matrices - cp.eye(3, dtype=cp.float32) * q[:, :, None, None]) / p[:, :, None, None]\n",
    "        r = cp.linalg.det(B) / 2\n",
    "        phi = cp.arccos(cp.clip(r, -1, 1)) / 3\n",
    "        del cov_matrices\n",
    "\n",
    "        eigvals_2 = q + 2 * p * cp.cos(phi + (2 * cp.pi / 3))\n",
    "        eigvals_2 = cp.asnumpy(eigvals_2)\n",
    "        eigvals_2[:y_top, :] = 0\n",
    "        eigvals_2[y_bottom:, :] = 0\n",
    "\n",
    "        neighbours = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "        strong_y, strong_x = np.where(eigvals_2 >= s)\n",
    "        weak_values = (eigvals_2 >= w) & (eigvals_2 < s).astype(np.uint8) \n",
    "        return_array = np.zeros_like(eigvals_2, dtype=cp.uint8)\n",
    "        \n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "        queue = deque(zip(strong_y, strong_x))\n",
    "        while queue:\n",
    "            y, x = queue.popleft() \n",
    "            return_array[y, x] = 1\n",
    "            for dy, dx in neighbours:\n",
    "                ny, nx = y + dy, x + dx\n",
    "                if 0 <= ny < weak_values.shape[0] and 0 <= nx < weak_values.shape[1]:\n",
    "                    if weak_values[ny, nx]:\n",
    "                        weak_values[ny, nx] = 0 \n",
    "                        queue.append((ny, nx)) \n",
    "        del weak_values \n",
    "\n",
    "        self.PCA_mask = return_array\n",
    "\n",
    "    def _get_props(self, distribution1, distribution2):\n",
    "            properties = []\n",
    "            return_properties = []\n",
    "\n",
    "            variance =  np.var(distribution1) *((10/self.window_size)**2)\n",
    "            skw = skew(distribution1)\n",
    "            kurt = kurtosis(distribution1)\n",
    "            mean = np.sqrt(np.mean(np.abs(distribution2)))\n",
    "            max = np.sqrt(np.max(np.abs(distribution2)))\n",
    "\n",
    "            \n",
    "            properties.append(variance)\n",
    "            properties.append(skw)\n",
    "            properties.append(kurt)\n",
    "\n",
    "            norm = np.linalg.norm(properties)\n",
    "            \n",
    "            if norm > 0:\n",
    "                properties = [x / norm for x in properties]\n",
    "\n",
    "            z = properties[0]\n",
    "\n",
    "            return_properties.append(mean)\n",
    "            return_properties.append(max)\n",
    "            return_properties.append(np.abs(properties[1]))\n",
    "            return_properties.append(np.arctan(z)/(np.pi/2))\n",
    "\n",
    "\n",
    "            return return_properties, np.uint8(properties[1] > 0)\n",
    "        \n",
    "    def _load_DBSCAN_model(self):\n",
    "            if (self.tree is None) or (self.labels is None):\n",
    "                with open(self.DBSCAN_model_path, 'rb') as f:\n",
    "                    self.tree, self.labels = pickle.load(f)\n",
    "\n",
    "    def _build_processor(self):\n",
    "        self._find_files()\n",
    "        self._validate_files()\n",
    "        self._load_component_parameters()\n",
    "        self._load_lidar_data()\n",
    "        self._interpolate_x_positions()\n",
    "        \n",
    "        mask = (self.point_cloud[:,0] <= self.x_start) & (self.point_cloud[:,0] >= self.x_stop) & ((self.point_cloud[:,1]) <= (self.window_size + 5 + self.y_shift)) & ((self.point_cloud[:,1]) >=  -(self.window_size + 5 - self.y_shift))\n",
    "        self.point_cloud = self.point_cloud[mask]\n",
    "        self.intensity_cloud = self.intensity_cloud[mask]\n",
    "        self.point_cloud[:,1] -= self.y_shift\n",
    "        self.intensity_cloud[:,1] -= self.y_shift\n",
    "\n",
    "        min_intensity = np.nanmax(self.intensity_cloud[:,2])\n",
    "        min_z = np.nanmax(self.point_cloud[:,2])\n",
    "\n",
    "        x_values = np.unique(self.point_cloud[:,0])\n",
    "        y_values = np.unique(self.point_cloud[:,1]) \n",
    "        x_range = len(x_values)\n",
    "\n",
    "        if self.upsample_ratio > 1:\n",
    "            index_step = (np.nanmedian(np.diff(x_values))) / self.upsample_ratio \n",
    "            index_steps = np.arange(int(round(np.nanmin(x_values)))/index_step - 1,int(round((np.nanmax(x_values))/index_step)+1)) * index_step\n",
    "            x_range = len(index_steps)\n",
    "            known_indices = np.unique([np.argmin(np.abs(index_steps - x)) for x in x_values])\n",
    "            unique_indices, unique_idx = np.unique(known_indices, return_index=True)\n",
    "            known_indices = unique_indices\n",
    "            known_x_values = x_values[unique_idx]\n",
    "            all_indices = np.arange(x_range)\n",
    "            interp_func = interp1d(known_indices, known_x_values, kind=\"linear\", fill_value=\"extrapolate\", bounds_error=False)\n",
    "            x_values = interp_func(all_indices)\n",
    "        else: self.upsample_ratio = 1\n",
    "        \n",
    "        x_value_dict = {x: index for index,x in enumerate(x_values)}\n",
    "        y_value_dict = {y: index for index,y in enumerate(y_values)}\n",
    "\n",
    "        point_array = np.full((len(y_values), x_range),np.nan)\n",
    "        intensity_array = np.full((len(y_values), x_range),np.nan)\n",
    "        point_dictionary = {(row[0],row[1]): (index, row[2]) for index,row in enumerate(self.point_cloud)}\n",
    "        intensity_dictionary = {(row[0],row[1]): (index, row[2]) for index,row in enumerate(self.intensity_cloud)}\n",
    "\n",
    "        for x in range (x_range):\n",
    "            for y in range (len(y_values)):\n",
    "                x_val = x_values[x]\n",
    "                y_val = y_values[y]\n",
    "\n",
    "                point_z = point_dictionary.get((x_val,y_val))\n",
    "                intensity_z = intensity_dictionary.get((x_val,y_val))\n",
    "\n",
    "                if point_z is not None:\n",
    "                    point_array[y,x] = point_z[1]\n",
    "                else:\n",
    "                    point_array[y,x] = min_z\n",
    "                if intensity_z is not None:\n",
    "                    intensity_array[y,x] = intensity_z[1]\n",
    "                else:\n",
    "                    intensity_array[y,x] = min_intensity\n",
    "\n",
    "        if self.upsample_ratio > 1:\n",
    "            for y in range(len(y_values)):\n",
    "\n",
    "                interpolated_z_values= []\n",
    "                interpolated_i_values = []\n",
    "\n",
    "                for dy in [-4,-3,-2,-1,0,1,2,3,4]:\n",
    "                    y_dy = y + dy\n",
    "                    if (y_dy >= 0) & (y_dy < len(y_values)):\n",
    "                        known_z = point_array[y_dy,known_indices]\n",
    "                        known_i = intensity_array[y_dy,known_indices]\n",
    "                        known_x = x_values[known_indices]\n",
    "                        mask = ~np.isnan(known_z)\n",
    "                        z_interp = interp1d(known_x[mask], known_z[mask], kind='linear', fill_value=\"extrapolate\")\n",
    "                        i_interp = interp1d(known_x[mask], known_i[mask], kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "                        interpolated_z_values.append(z_interp(x_values))\n",
    "                        interpolated_i_values.append(i_interp(x_values))\n",
    "\n",
    "                point_array[y, :] = np.mean(interpolated_z_values, axis=0)\n",
    "                intensity_array[y, :] = np.mean(interpolated_i_values, axis=0)\n",
    "\n",
    "        self.y_seed_point = np.argmin(np.abs(y_values))\n",
    "        self.point_cloud = point_array\n",
    "        self.intensity_cloud = intensity_array\n",
    "        self.x_to_i_dict = x_value_dict\n",
    "        self.i_to_x_list = x_values\n",
    "        self.y_to_i_dict = y_value_dict\n",
    "        self.i_to_y_list = y_values\n",
    "\n",
    "    def _create_edge_array(self,  **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        self._get_gradient()\n",
    "        self._create_PCA_mask()\n",
    "\n",
    "\n",
    "        y_values = self.i_to_y_list\n",
    "        neighbours = [(-1, 0), (0, -1), (0, 1), (1, 0)]\n",
    "    \n",
    "        result_array = np.zeros_like(self.gradient)\n",
    "\n",
    "        masked_gradient = self.gradient * self.PCA_mask \n",
    "        \n",
    "        strong_y, strong_x = np.where(masked_gradient >= self.strong_edge_threshold)\n",
    "        weak_edges = ((self.gradient >= self.weak_edge_threshold) & (masked_gradient < self.strong_edge_threshold)).astype(np.uint8) \n",
    "\n",
    "        queue = deque(zip(strong_y, strong_x))\n",
    "        while queue:\n",
    "            y, x = queue.popleft() \n",
    "            result_array[y, x] = 1\n",
    "            for dy, dx in neighbours:\n",
    "                ny, nx = y + dy, x + dx\n",
    "                if 0 <= ny < self.gradient.shape[0] and 0 <= nx < self.gradient.shape[1]:\n",
    "                    if weak_edges[ny, nx]:\n",
    "                        weak_edges[ny, nx] = 0 \n",
    "                        queue.append((ny, nx)) \n",
    "\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (6,22))\n",
    "        y_top = np.argmin(np.abs(y_values - self.window_size))\n",
    "        y_bottom = np.argmin(np.abs(y_values + self.window_size))\n",
    "        \n",
    "\n",
    "        result_array = cv2.morphologyEx(result_array, cv2.MORPH_CLOSE, kernel)\n",
    "        result_array[y_top, :] = 1\n",
    "        result_array[y_bottom, :] = 1\n",
    "\n",
    "        self.edge_array = np.where(result_array ==  1, 1 , np.nan)\n",
    "\n",
    "    def _define_sections(self, noise_threshold = 7,**kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        self._create_edge_array()\n",
    "        self._load_DBSCAN_model()\n",
    "        convolved_point_cloud = convolve(self.point_cloud,np.array([[-1],[-2],[0],[2],[1]]))\n",
    "        neighbours = [(-1, 0), (0, -1), (0, 1), (1, 0)]\n",
    "        nan_mask = np.isnan(self.edge_array) \n",
    "        \n",
    "        self.labeled_x_values = np.zeros(self.point_cloud.shape[1], dtype=int)\n",
    "\n",
    "        for x in range(self.edge_array.shape[1]):\n",
    "            center_x_vals = []\n",
    "            avg_z_vals = {}\n",
    "            avg_cz_vals = {}\n",
    "            size = 0\n",
    "            if nan_mask[self.y_seed_point, x]: \n",
    "                queue = deque([(self.y_seed_point, x)])\n",
    "                while queue:\n",
    "                    y, x = queue.popleft()\n",
    "                    size += 1\n",
    "\n",
    "                    if y == self.y_seed_point:\n",
    "                        center_x_vals.append(x)\n",
    "\n",
    "                    if y not in avg_z_vals.keys():\n",
    "                        avg_z_vals[y] = []\n",
    "                        avg_cz_vals[y] = []\n",
    "\n",
    "                    avg_z_vals[y].append(self.point_cloud[y,x])\n",
    "                    avg_cz_vals[y].append(convolved_point_cloud[y,x])\n",
    "                    nan_mask[y, x] = False  \n",
    "\n",
    "                    neighbors_to_add = [\n",
    "                        (ny, nx) for dy, dx in neighbours\n",
    "                        if (0 <= (ny := y + dy) < self.edge_array.shape[0] and \n",
    "                            0 <= (nx := x + dx) < self.edge_array.shape[1] and \n",
    "                            nan_mask[ny, nx])  \n",
    "                    ]\n",
    "                    queue.extend(neighbors_to_add) \n",
    "                    for ny, nx in neighbors_to_add:\n",
    "                        nan_mask[ny, nx] = False\n",
    "            \n",
    "            if size > 1000 * self.upsample_ratio:\n",
    "                avg_z_vals = [np.nanmedian(z_values) for z_values in avg_z_vals.values()]\n",
    "                avg_cz_vals = [np.nanmedian(z_values) for z_values in avg_cz_vals.values()]\n",
    "\n",
    "                v,b = self._get_props(avg_z_vals,avg_cz_vals)\n",
    "                closest_label = 0\n",
    "                if not np.any(np.isnan(v)):  \n",
    "                    dist, ind = self.tree.query([v], k=1)  \n",
    "\n",
    "                    if dist[0] <= noise_threshold: \n",
    "                        closest_label = self.labels[ind[0]] \n",
    "                        \n",
    "                        if closest_label == 2:\n",
    "                            closest_label += b\n",
    "                for x in center_x_vals:\n",
    "                    self.labeled_x_values[x] = closest_label\n",
    "\n",
    "    def _classify_rubble(self, x_window=1, y_window=4,**kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        x_window = x_window * self.upsample_ratio\n",
    "\n",
    "        pc = cp.array(self.point_cloud)\n",
    "        pc = pc[self.y_seed_point - y_window:self.y_seed_point + y_window + 1,:]\n",
    "\n",
    "        width = pc.shape[1]\n",
    "        y_range, x_range = 2 * y_window + 1, 2 * x_window + 1\n",
    "\n",
    "        x_n = cp.arange(-x_window, x_window + 1) /self.upsample_ratio  \n",
    "        y_n = cp.arange(-y_window, y_window + 1) /4.5 \n",
    "        valid_x = cp.repeat(x_n, y_range).flatten()\n",
    "        valid_y = cp.tile(y_n, x_range).flatten()\n",
    "\n",
    "        points = cp.lib.stride_tricks.sliding_window_view(\n",
    "            cp.pad(pc, ((0,0), (x_window,x_window)), mode='edge'), (y_range, x_range)\n",
    "        )\n",
    "\n",
    "        points = points.reshape(width, y_range*x_range)\n",
    "\n",
    "        points = cp.stack((\n",
    "            cp.broadcast_to(valid_x, (width, y_range*x_range)),\n",
    "            cp.broadcast_to(valid_y, (width, y_range*x_range)),\n",
    "            points), axis=2)\n",
    "        mean_vals = cp.mean(points[:,:,2], axis=(1), keepdims=True)\n",
    "        points[:,:,2] -= mean_vals\n",
    "\n",
    "        cov_matrices = cp.matmul(points.transpose(0,2,1), points) / (y_range * x_range - 1)\n",
    "\n",
    "        eigvals, eigvecs = cp.linalg.eigh(cov_matrices)\n",
    "\n",
    "        eigvecs = eigvecs[:, :, 0]\n",
    "\n",
    "        eigvals = eigvals[:,0]/(eigvals[:,0] + eigvals[:,1]  + eigvals[:,2]) \n",
    "        \n",
    "        x_offset = cp.abs(cp.arctan(eigvecs[:,0]/eigvecs[:,2]))\n",
    "        y_offset = cp.abs(cp.arctan(eigvecs[:,1]/eigvecs[:,2]))\n",
    "\n",
    "        results = {self.i_to_x_list[i]: self.labeled_x_values[i] if self.labeled_x_values[i] != 0 else [float(eigvals[i]),float(x_offset[i]),float(y_offset[i])] for i in range(len(self.i_to_x_list))}\n",
    "\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    " \n",
    "        self.rubble_classifications = results\n",
    "   \n",
    "    def _define_correction_windows(self, xrf_window_size=10, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        self._define_sections()\n",
    "        self._classify_rubble()\n",
    "\n",
    "        windows = {}\n",
    "\n",
    "        for x_start in range(0, round(np.nanmax(self.i_to_x_list)), xrf_window_size):\n",
    "            values = [self.rubble_classifications[i] for i, x in zip(self.i_to_x_list, self.i_to_x_list) \n",
    "            if x_start <= x < x_start + xrf_window_size]\n",
    "    \n",
    "            if values:\n",
    "                x_end = max(x for x in self.i_to_x_list if x_start <= x < x_start + xrf_window_size)\n",
    "                x_start = min(x for x in self.i_to_x_list if x_start <= x < x_start + xrf_window_size)\n",
    "            \n",
    "                labels = np.array([v for v in values if v in [1,2,3]])\n",
    "                rubble_points = [v for v in values if v not in [0,1,2,3]]\n",
    "                \n",
    "                rubble_points = np.array(rubble_points)\n",
    "            \n",
    "                half_perc = np.sum(labels == 1) / len(values)\n",
    "                empty_perc = np.sum(labels == 2) / len(values)\n",
    "                full_perc = np.sum(labels == 3) / len(values)\n",
    "                rubble_perc = 1 - half_perc - empty_perc - full_perc\n",
    "\n",
    "                if rubble_points.size > 0:\n",
    "                    avg_var = np.nanmean(rubble_points[:, 0])\n",
    "                    avg_x_offset = np.nanmean(rubble_points[:, 1])\n",
    "                    avg_y_offset = np.nanmean(rubble_points[:, 2])\n",
    "                else:\n",
    "                    avg_var = avg_x_offset = avg_y_offset = np.nan  \n",
    "\n",
    "                windows[(x_start, x_end)] = [half_perc, full_perc, empty_perc, rubble_perc, avg_var, avg_x_offset, avg_y_offset]\n",
    "\n",
    "        self.correction_windows = windows\n",
    "\n",
    "    def _save_correction_windows(self, xrf_window_size=10, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        if self.correction_windows == None:\n",
    "            self._define_correction_windows(xrf_window_size)\n",
    "        \n",
    "        os.makedirs(self.file_path, exist_ok=True)\n",
    "        \n",
    "        file_path = os.path.join(self.file_path, 'rubble_classification.pkl')\n",
    "        \n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(self.correction_windows, f)\n",
    "\n",
    "    def _plot_correction_windows(self, plot_point_cloud = False, width = 150, height = 7, dpi = 75, xrf_window_size=None,**kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        if  xrf_window_size is not None:\n",
    "            self._define_correction_windows(xrf_window_size)\n",
    "\n",
    "        if self.correction_windows is None:\n",
    "            self._define_correction_windows()\n",
    "\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "\n",
    "        colormap = [\"green\", \"blue\", \"purple\", \"red\"]\n",
    "        cmap1 = plt.cm.cool  \n",
    "        cmap2 = plt.cm.cool\n",
    "\n",
    "        all_vars = []\n",
    "        all_angles = []\n",
    "\n",
    "        for values in self.correction_windows.values():\n",
    "            var, x_angle, y_angle = values[4:7]\n",
    "            all_vars.append(var)\n",
    "            all_angles.append(x_angle)\n",
    "\n",
    "        min_var = np.nanmin(all_vars)\n",
    "        max_var = np.nanmax(all_vars)\n",
    "        min_angle = np.nanmin(all_angles)\n",
    "        max_angle = np.nanmax(all_angles)\n",
    "\n",
    "        if plot_point_cloud:\n",
    "            display = self.point_cloud\n",
    "        else:\n",
    "            display = self.intensity_cloud\n",
    "        ax.imshow(np.flipud(display), cmap=self.colourmap, interpolation='nearest', alpha=1, aspect= 'auto')\n",
    "\n",
    "        for (x_start, x_end), values in self.correction_windows.items():\n",
    "            half_perc, empty_perc, full_perc, rubble_perc = values[:4]\n",
    "\n",
    "            ratios = np.array([half_perc, empty_perc, full_perc, rubble_perc])\n",
    "            ratios /= ratios.sum()  \n",
    "\n",
    "            width = self.x_to_i_dict[x_end] - self.x_to_i_dict[x_start] + 1\n",
    "            bar_heights = ratios * 12  \n",
    "            bottoms = self.y_seed_point - 6 + np.insert(np.cumsum(bar_heights[:-1]), 0, 0)\n",
    "\n",
    "            for j in range(4):\n",
    "                ax.bar(self.x_to_i_dict[x_start], height=bar_heights[j], width=width,\n",
    "                    bottom=bottoms[j], color=colormap[j], alpha=0.65, align='edge')\n",
    "\n",
    "            if rubble_perc > 0:\n",
    "                var, x_angle, y_angle = values[4:]\n",
    "                norm1 = plt.Normalize(vmin=min_var, vmax=max_var)\n",
    "                norm2 = plt.Normalize(vmin=min_angle, vmax=max_angle)\n",
    "                color1 = cmap1(norm1(var))\n",
    "                color2 = cmap2(norm2(x_angle))\n",
    "                color3 = cmap2(norm2(y_angle))\n",
    "                extra_heights = 7.5\n",
    "\n",
    "                ax.bar(self.x_to_i_dict[x_start], height=extra_heights, width=width,\n",
    "                    bottom=0, color=color1, alpha=0.75, align='edge')\n",
    "                ax.bar(self.x_to_i_dict[x_start], height=extra_heights, width=width,\n",
    "                    bottom=extra_heights, color=color2, alpha=0.75, align='edge')\n",
    "                ax.bar(self.x_to_i_dict[x_start], height=extra_heights, width=width,\n",
    "                    bottom=extra_heights*2, color=color3, alpha=0.75, align='edge')\n",
    "\n",
    "        ax.set_xlabel(\"X index\",fontsize = 20)\n",
    "        ax.set_ylabel(\"Y index\",fontsize = 20)\n",
    "        ax.set_title(self.name, fontsize=35)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_gradient(self,width = 150, height = 7, dpi = 75):\n",
    "        if self.gradient is None:\n",
    "            self._get_gradient()\n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "        ax.imshow(cp.flipud(self.gradient), cmap='nipy_spectral', interpolation='nearest', alpha = 1, aspect = 'auto')\n",
    "        ax.set_xlabel(\"X index\",fontsize = 20)\n",
    "        ax.set_ylabel(\"Y index\",fontsize = 20)\n",
    "        ax.set_title(self.name, fontsize=35)\n",
    "\n",
    "    def _plot_PCA_mask(self,plot_point_cloud = False, plot_intensity_cloud = True, width = 150, height = 7, dpi = 75, colourmap = 'cool', **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        if self.PCA_mask is None:\n",
    "            self._create_PCA_mask()\n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "        a = 1\n",
    "        if plot_intensity_cloud:\n",
    "            ax.imshow(cp.flipud(self.intensity_cloud), cmap=self.colourmap, interpolation='nearest', alpha = 1, aspect = 'auto')\n",
    "            a = 0.25\n",
    "        if plot_point_cloud:\n",
    "            ax.imshow(cp.flipud(self.point_cloud), cmap=self.colourmap, interpolation='nearest', alpha = 1, aspect = 'auto')\n",
    "            a = 0.25\n",
    "        ax.imshow(cp.flipud(self.PCA_mask), cmap=self.colourmap, interpolation='nearest', alpha = a, aspect = 'auto')\n",
    "        ax.set_xlabel(\"X index\")\n",
    "        ax.set_ylabel(\"Y index\")\n",
    "        ax.set_title(self.name)\n",
    "\n",
    "    def _plot_sections(self,plot_point_cloud = False, plot_intensity_cloud = True, width = 150, height = 7, dpi = 75, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        if self.edge_array is None:\n",
    "            self._create_edge_array()\n",
    "        sections = self.edge_array.copy()\n",
    "    \n",
    "        neighbours = [(-1, 0), (0, -1), (0, 1), (1, 0)]  \n",
    "        nan_mask = np.isnan(sections)  \n",
    "\n",
    "        int = 2\n",
    "        for x in range(sections.shape[1]):\n",
    "            if nan_mask[self.y_seed_point, x]: \n",
    "                queue = deque([(self.y_seed_point, x)])\n",
    "                while queue:\n",
    "                    y, x = queue.popleft()\n",
    "                    sections[y, x] = int \n",
    "                    nan_mask[y, x] = False  \n",
    "                    neighbors_to_add = [(ny, nx) for dy, dx in neighbours\n",
    "                        if (0 <= (ny := y + dy) < sections.shape[0] and \n",
    "                            0 <= (nx := x + dx) < sections.shape[1] and nan_mask[ny, nx])]\n",
    "                    queue.extend(neighbors_to_add) \n",
    "                    for ny, nx in neighbors_to_add:\n",
    "                        nan_mask[ny, nx] = False \n",
    "            int += 1\n",
    "        \n",
    "        sections = np.where(sections > 1, sections, np.nan)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "        a = 1\n",
    "        if plot_intensity_cloud:\n",
    "            ax.imshow(cp.flipud(self.intensity_cloud), cmap=self.colourmap, interpolation='nearest', alpha = 1, aspect= 'auto')\n",
    "            a = 0.8\n",
    "        if plot_point_cloud:\n",
    "            ax.imshow(cp.flipud(self.point_cloud), cmap=self.colourmap, interpolation='nearest', alpha = 1, aspect= 'auto')\n",
    "            a = 0.8\n",
    "        ax.imshow(cp.flipud(sections), cmap='cool', interpolation='nearest', alpha = a, aspect= 'auto')\n",
    "        ax.set_xlabel(\"X index\")\n",
    "        ax.set_ylabel(\"Y index\")\n",
    "        ax.set_title(self.name)\n",
    "\n",
    "    def _print_instance_attributes(self):\n",
    "        for attr, value in vars(self).items():\n",
    "            print(f\"{attr}: {'None' if value is None else 'Not None'}\")\n",
    "\n",
    "    def _view_point_cloud(self, view_intensity = False):\n",
    "        x = (list)(self.i_to_x_list) * len(self.i_to_y_list)\n",
    "        y = np.repeat(self.i_to_y_list, len(self.i_to_x_list))\n",
    "        if view_intensity:\n",
    "            z = self.intensity_cloud.flatten()\n",
    "        else:\n",
    "            z = self.point_cloud.flatten()\n",
    "\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        points = np.column_stack((x, y, -z))\n",
    "        pcd.points = o3d.utility.Vector3dVector(points)\n",
    "        \n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "\n",
    "    def _plot_point_cloud(self,width = 150, height = 7, dpi = 75, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "        ax.imshow(cp.flipud(self.point_cloud), cmap=self.colourmap, interpolation='nearest', alpha = 1, aspect = 'auto')\n",
    "        ax.set_xlabel(\"X index\")\n",
    "        ax.set_ylabel(\"Y index\")\n",
    "        ax.set_title(self.name)\n",
    "\n",
    "    def _plot_intensity_cloud(self,width = 150, height = 7, dpi = 75, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if value is not None and hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "        ax.imshow(cp.flipud(self.intensity_cloud), cmap=self.colourmap, interpolation='nearest', alpha = 1, aspect = 'auto')\n",
    "        ax.set_xlabel(\"X index\")\n",
    "        ax.set_ylabel(\"Y index\")\n",
    "        ax.set_title(self.name)\n",
    "\n",
    "    def _get_data_cube(self,intensity = False):\n",
    "        x = (list)(self.i_to_x_list) * len(self.i_to_y_list)\n",
    "        y = np.repeat(self.i_to_y_list, len(self.i_to_x_list))\n",
    "        if intensity:\n",
    "            z = self.intensity_cloud.flatten()\n",
    "        else:\n",
    "            z = self.point_cloud.flatten()\n",
    "\n",
    "        return np.column_stack((x, y, z))\n",
    "\n",
    "    def _save_to_las(self):\n",
    "        x = (list)(self.i_to_x_list) * len(self.i_to_y_list)\n",
    "        y = np.repeat(self.i_to_y_list, len(self.i_to_x_list))\n",
    "        i = self.intensity_cloud.flatten()\n",
    "        z = self.point_cloud.flatten()\n",
    "\n",
    "        file_path = os.path.join(self.file_path, \"point_cloud.las\")\n",
    "\n",
    "        header = laspy.LasHeader(point_format=1, version=\"1.2\") \n",
    "        las = laspy.LasData(header)\n",
    "\n",
    "        las.x = np.array(x)\n",
    "        las.y = np.array(y)\n",
    "        las.z = np.array(z)\n",
    "        las.intensity = np.array(i)\n",
    "\n",
    "        las.write(file_path)\n",
    "\n",
    "    def _print_scan_limits(self, translated = False):\n",
    "        if translated:\n",
    "            data = self.translated_cloud_limits\n",
    "            print(f\"limits in machine coordinates for {self.name}:\")\n",
    "        else:\n",
    "            data = self.original_cloud_limits\n",
    "            print(f\"limits in lidar coordinates for {self.name}:\")\n",
    "\n",
    "        print(f\" x value range: {data[0]}mm to {data[1]}mm, number of lines in the x direction: {data[2]}\")\n",
    "        print(f\" y value range: {data[3]}mm to {data[4]}mm, number of lines in the y direction: {data[5]}\")\n",
    "        print(f\" z value range: {data[6]}mm to {data[7]}mm, total number of points: {data[8]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r\"C:\\Users\\eashenhurst\\Desktop\\local macassa\"\n",
    "filter = \"10mm\"\n",
    "\n",
    "#root_dir = r\"\\\\192.168.1.63\\g\\CoreScans\"\n",
    "#filter = \"11\"\n",
    "\n",
    "\n",
    "#root_dir = r\"C:\\Users\\eashenhurst\\Desktop\\auzzy rocks\"\n",
    "#filter = \"Part_1\"\n",
    "\n",
    "paths_list = []\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "        for dir_name in dirs:\n",
    "            if dir_name.startswith(\"Core\"):\n",
    "                folder = os.path.join(root,dir_name)\n",
    "                if filter in folder:\n",
    "                    paths_list.append(folder)\n",
    "\n",
    "\n",
    "model_path = 'cluster_kd_tree.pkl'\n",
    "\n",
    "processors = []\n",
    "for folder in paths_list:\n",
    "    for i in range (0,1):\n",
    "        try:\n",
    "            processor = lidarProcessor(folder, model_path, upsample_ratio = i, name = (folder.split('\\\\')[-1] + f\" upsample_ratio = {i}\"))\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"   Error: {e}\")\n",
    "        else:  \n",
    "            processors.append(processor)\n",
    "            print(processor.name)\n",
    "\n",
    "print(f\"{len(processors)} Lidar Processors created\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for LP in processors:\n",
    "    LP._print_scan_limits()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LP = lidarProcessor(r\"C:\\Users\\eashenhurst\\Desktop\\local macassa\\Box1\\AdaptiveZ_10mm\\Core_000_Box_001_of_020_Part_4_of_4\", window_size= 1600, upsample_ratio= 0)\n",
    "\n",
    "LP._view_point_cloud()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LP_env",
   "language": "python",
   "name": "lp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
