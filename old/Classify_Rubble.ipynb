{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import pickle\n",
    "import matplotlib.pyplot    as plt\n",
    "import numpy                as np\n",
    "import cupy                 as cp\n",
    "from scipy.stats       import skew, kurtosis\n",
    "from scipy.ndimage     import convolve\n",
    "from cupyx.scipy.ndimage import convolve as cpconvolve\n",
    "from collections import deque\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.neighbors import KDTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables\n",
    "part_path = r\"C:\\Users\\eashenhurst\\Desktop\\local macassa\"\n",
    "\n",
    "window_height = 10\n",
    "\n",
    "incident_beam_x_angle = 0\n",
    "incident_beam_y_angle = 0\n",
    "\n",
    "plot_sections = True\n",
    "plot_labels = False\n",
    "plot_rubble_sections = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "\n",
    "#vars\n",
    "\n",
    "label, heights, convolved_heights, intensity, x_to_i, i_to_x, y_to_i, i_to_y, sectioned, final = 0,1,2,3,4,5,6,7,8,9\n",
    "\n",
    "global tree\n",
    "global labels\n",
    "\n",
    "#kernels\n",
    "gauss =  cp.array([\n",
    "    [1,  4,  7,  4,  1],\n",
    "    [4, 16, 26, 16,  4],\n",
    "    [7, 26, 41, 26,  7],\n",
    "    [4, 16, 26, 16,  4],\n",
    "    [1,  4,  7,  4,  1]\n",
    "]) / 273\n",
    "\n",
    "gauss_y =  cp.array([\n",
    "    [1],\n",
    "    [4],\n",
    "    [7],\n",
    "    [4],\n",
    "    [1]\n",
    "]) / 17\n",
    "\n",
    "sobel_y = cp.array([\n",
    "        [-20.75,],\n",
    "        [ -11.6,],\n",
    "        [ -6.27,],\n",
    "        [    -2,],\n",
    "        [     0,],\n",
    "        [     2,],\n",
    "        [  6.27,],\n",
    "        [  11.6,],\n",
    "        [ 20.75,]\n",
    "])\n",
    "sobel_x = sobel_y.T\n",
    "\n",
    "def interpolate_x(table_file,lidar_file):\n",
    "    with open(table_file) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        positions = []\n",
    "        table_timestamps = []\n",
    "\n",
    "        char = '-'\n",
    "\n",
    "        for line in lines:\n",
    "            if (\"+\") in line:\n",
    "                char = '+'\n",
    "            time = line.split('T')[1]\n",
    "            x = float(line.split(',')[0])\n",
    "            parts = time.split(':')\n",
    "            t = float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2].split(char)[0]) \n",
    "            positions.append(x)\n",
    "            table_timestamps.append(t)\n",
    "\n",
    "    ordered_positions = []\n",
    "    ordered_timestamps = []\n",
    "\n",
    "    decreasing = (positions[-1] - positions[0]) < 0 \n",
    "\n",
    "    if decreasing:\n",
    "        for i in range(len(positions)-1):\n",
    "            if positions[i + 1] < positions[i]:\n",
    "                ordered_positions.append(positions[i])\n",
    "                ordered_timestamps.append(table_timestamps[i])\n",
    "    else:\n",
    "         for i in range(len(positions)-1):\n",
    "            if positions[i + 1] > positions[i]:\n",
    "                ordered_positions.append(positions[i])\n",
    "                ordered_timestamps.append(table_timestamps[i])\n",
    "\n",
    "\n",
    "    with open(lidar_file) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        lidar_timestamps = []\n",
    "\n",
    "        time = lines[0].split('T')[1]\n",
    "        parts = time.split(':')\n",
    "        min_t = float(parts[1]) * 60  + float(parts[0]) * 3600 + float(parts[2].split(char)[0]) \n",
    "\n",
    "        for line in lines:\n",
    "            time = line.split('T')[1]\n",
    "            parts = time.split(':')\n",
    "            t = float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2].split(char)[0]) - min_t\n",
    "            lidar_timestamps.append(t)\n",
    "\n",
    "\n",
    "    lidar_timestamps = np.array(lidar_timestamps)\n",
    "    ordered_timestamps = np.array(ordered_timestamps)[::-1]\n",
    "    ordered_timestamps -= min_t\n",
    "    ordered_positions = ordered_positions[::-1]\n",
    "\n",
    "    ordered_timestamps = np.concatenate([ordered_timestamps[:2], ordered_timestamps[-2:]])\n",
    "    ordered_positions = np.concatenate([ordered_positions[:2], ordered_positions[-2:]])\n",
    "\n",
    "    interp_func = interp1d(ordered_timestamps, ordered_positions, kind=\"linear\", fill_value=(ordered_positions[-1], ordered_positions[0]), bounds_error=False) \n",
    "\n",
    "    interpolated = interp_func(lidar_timestamps)\n",
    "            \n",
    "    return interpolated\n",
    "\n",
    "def get_point_cloud(paths, y_window = window_height + 5, upsample_ratio = 2):\n",
    "    print(\"loading\",end = \"... \")\n",
    "\n",
    "    x_start = 50000\n",
    "    x_stop = 0\n",
    "    y_offset = 0\n",
    "    with open(paths[1]) as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                if \"XRAY_DPP[Acquisition]#0.X.Start:\" in line:\n",
    "                    x_start = (float)(line.split(\"XRAY_DPP[Acquisition]#0.X.Start:\")[1].strip())\n",
    "                if \"XRAY_DPP[Acquisition]#0.X.Stop:\" in line:\n",
    "                    x_stop = (float)(line.split(\"XRAY_DPP[Acquisition]#0.X.Stop:\")[1].strip())\n",
    "                if \"XRAY_DPP[Acquisition]#0.Y.Start:\" in line:\n",
    "                    y_offset = (float)(line.split(\"XRAY_DPP[Acquisition]#0.Y.Start:\")[1].strip())\n",
    "                    print(f\"y_offset: {y_offset}\")\n",
    "\n",
    "    with open(paths[2]) as file:\n",
    "        lines = file.readlines()\n",
    "        transformation_matrix = np.array([list(map(float, line.strip().split(\",\"))) for line in lines])\n",
    "\n",
    "    imread = lambda fn: cv2.imread(fn, cv2.IMREAD_ANYDEPTH)\n",
    "    \n",
    "    point_cloud  = np.fromfile(paths[3], dtype=np.float32).reshape(-1, 3) \n",
    "\n",
    "    interpolated_x = interpolate_x(paths[5],paths[6])\n",
    "    interpolated_x = np.repeat(interpolated_x, len(np.unique(point_cloud[:, 1])))\n",
    "\n",
    "    point_cloud[:,0] = interpolated_x\n",
    "\n",
    "    intensity_map = imread(paths[4])\n",
    "    \n",
    "    print(f\"{paths[0]} loaded, {point_cloud.shape[0]} points\")\n",
    "    print(\"trimming\",end = \"... \")\n",
    "    \n",
    "    intensity_values = np.reshape(intensity_map, (-1, 1))\n",
    "    print(np.nanmax(intensity_values))\n",
    "    intensity_cloud = np.hstack((point_cloud[:,:2], intensity_values))\n",
    "\n",
    "    point_cloud = (np.hstack((point_cloud, np.ones((point_cloud.shape[0], 1)))) @ transformation_matrix.T)[:,:3]\n",
    "    intensity_cloud = (np.hstack((intensity_cloud, np.ones((intensity_cloud.shape[0], 1)))) @ transformation_matrix.T)[:,:3]\n",
    "\n",
    "    mask = (point_cloud[:,0] <= x_start) & (point_cloud[:,0] >= x_stop) & ((np.abs(point_cloud[:,1])) <= y_window)\n",
    "    point_cloud = point_cloud[mask]\n",
    "    intensity_cloud = intensity_cloud[mask]\n",
    "\n",
    "    min_intensity = np.nanmax(intensity_cloud[:,2])\n",
    "    min_z = np.nanmax(point_cloud[:,2])\n",
    "    \n",
    "    print(f\"trimmed to {point_cloud.shape[0]} points\")\n",
    "\n",
    "    print(\"converting to arrays\",end = \"... \")\n",
    "\n",
    "    minimum_x = point_cloud[np.argmin(np.abs(point_cloud[:,0] - x_stop)),0]\n",
    "\n",
    "    point_cloud[:,0] -= minimum_x\n",
    "    intensity_cloud[:,0] -= minimum_x\n",
    "    \n",
    "    x_values = np.unique(point_cloud[:,0])\n",
    "    y_values = np.unique(point_cloud[:,1]) \n",
    "\n",
    "    x_range = len(x_values)\n",
    "\n",
    "    if upsample_ratio > 1:\n",
    "        index_step = (np.nanmedian(np.diff(x_values))) / upsample_ratio \n",
    "        index_steps = np.arange(int(round(np.nanmax(x_values)/index_step))+1) * index_step\n",
    "\n",
    "        x_range = len(index_steps)\n",
    "        x_value_dict = {x: np.argmin(np.abs(index_steps - x)) for x in x_values}\n",
    "        known_indices = list(x_value_dict.values())\n",
    "        known_x_values = np.array(list(x_value_dict.keys()), dtype=float)\n",
    "        all_indices = np.arange(x_range)\n",
    "\n",
    "        interp_func = interp1d(known_indices, known_x_values, kind=\"linear\", fill_value=\"extrapolate\")\n",
    "        x_values = interp_func(all_indices)\n",
    "    \n",
    "    x_value_dict = {x: index for index,x in enumerate(x_values)}\n",
    "    y_value_dict = {y: index for index,y in enumerate(y_values)}\n",
    "\n",
    "    point_array = np.full((len(y_values), x_range),np.nan)\n",
    "    intensity_array = np.full((len(y_values), x_range),np.nan)\n",
    "    point_dictionary = {(row[0],row[1]): (index, row[2]) for index,row in enumerate(point_cloud)}\n",
    "    intensity_dictionary = {(row[0],row[1]): (index, row[2]) for index,row in enumerate(intensity_cloud)}\n",
    "\n",
    "    for x in range (x_range):\n",
    "        for y in range (len(y_values)):\n",
    "            x_val = x_values[x]\n",
    "            y_val = y_values[y]\n",
    "\n",
    "            point_z = point_dictionary.get((x_val,y_val))\n",
    "            intensity_z = intensity_dictionary.get((x_val,y_val))\n",
    "\n",
    "            if point_z is not None:\n",
    "                point_array[y,x] = point_z[1]\n",
    "            else:\n",
    "                point_array[y,x] = min_z\n",
    "            if intensity_z is not None:\n",
    "                intensity_array[y,x] = intensity_z[1]\n",
    "            else:\n",
    "                intensity_array[y,x] = min_intensity\n",
    "\n",
    "    print(f\"arrays built\")\n",
    "    if upsample_ratio > 1:\n",
    "        print(\"upsampling\",end = \"... \")\n",
    "\n",
    "        for y in range(len(y_values)):\n",
    "\n",
    "            interpolated_z_values= []\n",
    "            interpolated_i_values = []\n",
    "\n",
    "            for dy in [-1,0,1]:\n",
    "                y_dy = y + dy\n",
    "                if (y_dy >= 0) & (y_dy < len(y_values)):\n",
    "                    known_z = point_array[y_dy,known_indices]\n",
    "                    known_i = intensity_array[y_dy,known_indices]\n",
    "                    known_x = x_values[known_indices]\n",
    "                    mask = ~np.isnan(known_z)\n",
    "                    z_interp = interp1d(known_x[mask], known_z[mask], kind='linear', fill_value=\"extrapolate\")\n",
    "                    i_interp = interp1d(known_x[mask], known_i[mask], kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "                    interpolated_z_values.append(z_interp(x_values))\n",
    "                    interpolated_i_values.append(i_interp(x_values))\n",
    "\n",
    "            point_array[y, :] = np.mean(interpolated_z_values, axis=0)\n",
    "            intensity_array[y, :] = np.mean(interpolated_i_values, axis=0)\n",
    "    \n",
    "    kernel_y = np.array([[-1],[-2],[0],[2],[1]])\n",
    "    convolved_array = np.abs(convolve(point_array,kernel_y))\n",
    "\n",
    "    print(f\"upsampled to {point_array.size} points\")\n",
    "    print(f\"{paths[0]} finished \\n\")\n",
    "\n",
    "    return [paths[0],point_array, convolved_array, intensity_array, x_value_dict, x_values, y_value_dict, y_values]\n",
    "\n",
    "def get_props(distribution1, distribution2):\n",
    "    properties = []\n",
    "    return_properties = []\n",
    "\n",
    "    variance =  np.var(distribution1)\n",
    "    skw = skew(distribution1)\n",
    "    kurt = kurtosis(distribution1)\n",
    "    mean = np.sqrt(np.mean(distribution2))\n",
    "    max = np.sqrt(np.max(distribution2))\n",
    "\n",
    "    \n",
    "    properties.append(variance)\n",
    "    properties.append(skw)\n",
    "    properties.append(kurt)\n",
    "\n",
    "    norm = np.linalg.norm(properties)\n",
    "    \n",
    "    if norm > 0:\n",
    "        properties = [x / norm for x in properties]\n",
    "\n",
    "    z = properties[0]\n",
    "\n",
    "    return_properties.append(mean)\n",
    "    return_properties.append(max)\n",
    "    return_properties.append(np.abs(properties[1]))\n",
    "    return_properties.append(np.arctan(z)/(np.pi/2))\n",
    "\n",
    "\n",
    "    return return_properties, np.uint8(properties[1] > 0)\n",
    "\n",
    "def classify_fhe(distribution, distribution2, noise_threshold = 15):\n",
    "    v,b = get_props(distribution,distribution2)\n",
    "    closest_label = 0\n",
    "    if not np.any(np.isnan(v)):  \n",
    "        dist, ind = tree.query([v], k=1)  \n",
    "\n",
    "        if dist[0][0] <= noise_threshold: \n",
    "            closest_label = labels[ind[0][0]] \n",
    "            \n",
    "            if closest_label == 2:\n",
    "                closest_label += b \n",
    "        \n",
    "    return closest_label\n",
    "\n",
    "def get_gradient(data):\n",
    "    array_i = cp.array(np.log(np.abs(data[intensity])), dtype=cp.float32)\n",
    "    array_z = cp.array(data[heights], dtype=cp.float32)\n",
    "    y_values = data[i_to_y]\n",
    "\n",
    "    y_grad_i = cp.abs(cpconvolve(array_i, sobel_y))\n",
    "    x_grad_i = cp.abs(cpconvolve(array_i, sobel_x))\n",
    "    magnitude_i = cp.sqrt(x_grad_i**2 + y_grad_i**2)\n",
    "    y_grad_i = cp.abs(cpconvolve(array_i, sobel_y))\n",
    "    magnitude_i = cp.sqrt(x_grad_i**2 + y_grad_i**2)\n",
    "\n",
    "    x_grad_z = cp.abs(cpconvolve(array_z, sobel_y))\n",
    "    y_grad_z = cp.abs(cpconvolve(array_z, sobel_x))\n",
    "    magnitude_z = cp.sqrt(x_grad_z**2 + y_grad_z**2)\n",
    "    y_grad_z = cp.abs(cpconvolve(magnitude_z, sobel_x))\n",
    "    magnitude_z = cp.sqrt(x_grad_z**2 + y_grad_z**2)\n",
    "\n",
    "\n",
    "    magnitude = cp.sqrt(magnitude_i * magnitude_z)\n",
    "\n",
    "    y_bottom = np.argmin(np.abs(y_values - window_height))\n",
    "    y_top = np.argmin(np.abs(y_values + window_height))\n",
    "\n",
    "    magnitude[:y_top, :] = 0\n",
    "    magnitude[y_bottom:, :] = 0\n",
    "    \n",
    "    return cp.asnumpy(magnitude)\n",
    "\n",
    "def PCA(data, y_window=9, x_window=2, s = 0.001, w = 0.0005):\n",
    "    array = cp.array(data[heights], dtype=cp.float32)\n",
    "    y_values = data[i_to_y]\n",
    "    height, width = array.shape\n",
    "    y_range, x_range = 2 * y_window + 1, 2 * x_window + 1\n",
    "\n",
    "    y_bottom = np.argmin(np.abs(y_values - window_height))\n",
    "    y_top = np.argmin(np.abs(y_values + window_height))\n",
    "\n",
    "    x_n = cp.arange(-x_window, x_window + 1) /2\n",
    "    y_n = cp.arange(-y_window, y_window + 1) /4.5\n",
    "    valid_x = cp.repeat(x_n, y_range).flatten()\n",
    "    valid_y = cp.tile(y_n, x_range).flatten()\n",
    "\n",
    "    points = cp.lib.stride_tricks.sliding_window_view(\n",
    "        cp.pad(array, ((y_window,y_window), (x_window,x_window)), mode='edge'), (y_range, x_range)\n",
    "    )\n",
    "    del array\n",
    "\n",
    "    points = points.reshape(height,width, y_range*x_range)\n",
    "    \n",
    "    points = cp.stack((\n",
    "        cp.broadcast_to(valid_x, (height,width, y_range*x_range)),\n",
    "        cp.broadcast_to(valid_y, (height,width, y_range*x_range)),\n",
    "        points), axis=2)\n",
    "    mean_vals = cp.mean(points[:,:,2,:], axis=(2), keepdims=True)\n",
    "    points[:,:,2,:] -= mean_vals\n",
    "\n",
    "    cov_matrices = cp.matmul(points, points.transpose(0, 1, 3, 2)) / (points.shape[3] - 1)\n",
    "    del points \n",
    "\n",
    "    a, b, c, d, e, f, g, h, i = (\n",
    "        cov_matrices[:, :, 0, 0], cov_matrices[:, :, 0, 1], cov_matrices[:, :, 0, 2],\n",
    "        cov_matrices[:, :, 1, 0], cov_matrices[:, :, 1, 1], cov_matrices[:, :, 1, 2],\n",
    "        cov_matrices[:, :, 2, 0], cov_matrices[:, :, 2, 1], cov_matrices[:, :, 2, 2]\n",
    "    )\n",
    "\n",
    "    p1 = b**2 + c**2 + f**2\n",
    "    q = (a + e + i) / 3\n",
    "    p2 = (a - q)**2 + (e - q)**2 + (i - q)**2 + 2 * p1\n",
    "    p = cp.sqrt(p2 / 6)\n",
    "    \n",
    "    B = (cov_matrices - cp.eye(3, dtype=cp.float32) * q[:, :, None, None]) / p[:, :, None, None]\n",
    "    r = cp.linalg.det(B) / 2\n",
    "    phi = cp.arccos(cp.clip(r, -1, 1)) / 3\n",
    "    del cov_matrices\n",
    "\n",
    "    eigvals_2 = q + 2 * p * cp.cos(phi + (2 * cp.pi / 3))\n",
    "    eigvals_2 = cp.asnumpy(eigvals_2)\n",
    "    eigvals_2[:y_top, :] = 0\n",
    "    eigvals_2[y_bottom:, :] = 0\n",
    "\n",
    "    neighbours = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "    strong_y, strong_x = np.where(eigvals_2 >= s)\n",
    "    weak_values = (eigvals_2 >= w) & (eigvals_2 < s).astype(np.uint8) \n",
    "    return_array = np.zeros_like(eigvals_2, dtype=cp.uint8)\n",
    "    \n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    queue = deque(zip(strong_y, strong_x))\n",
    "    while queue:\n",
    "        y, x = queue.popleft() \n",
    "        return_array[y, x] = 1\n",
    "        for dy, dx in neighbours:\n",
    "            ny, nx = y + dy, x + dx\n",
    "            if 0 <= ny < weak_values.shape[0] and 0 <= nx < weak_values.shape[1]:\n",
    "                if weak_values[ny, nx]:\n",
    "                    weak_values[ny, nx] = 0 \n",
    "                    queue.append((ny, nx)) \n",
    "    del weak_values \n",
    "\n",
    "    return return_array\n",
    "\n",
    "def get_edges(data,s = 80, w = 30, y_window=9, x_window=2, s_e = 0.01, w_e = 0.0075):\n",
    "    mask = PCA(data, y_window, x_window, s = s_e, w = w_e)\n",
    "    NMS_magnitude = get_gradient(data)\n",
    "\n",
    "    y_values = data[i_to_y]\n",
    "    neighbours = [(-1, 0), (0, -1), (0, 1), (1, 0)]\n",
    "   \n",
    "    return_array = np.zeros_like(NMS_magnitude)\n",
    "\n",
    "    NMS_array_masked = NMS_magnitude * mask \n",
    "    \n",
    "    strong_y, strong_x = np.where(NMS_array_masked >= s)\n",
    "    weak_edges = (NMS_magnitude >= w) & (NMS_array_masked < s).astype(np.uint8) \n",
    "\n",
    "    queue = deque(zip(strong_y, strong_x))\n",
    "    while queue:\n",
    "        y, x = queue.popleft() \n",
    "        return_array[y, x] = 1\n",
    "        for dy, dx in neighbours:\n",
    "            ny, nx = y + dy, x + dx\n",
    "            if 0 <= ny < mask.shape[0] and 0 <= nx < mask.shape[1]:\n",
    "                if weak_edges[ny, nx]:\n",
    "                    weak_edges[ny, nx] = 0 \n",
    "                    queue.append((ny, nx)) \n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (6,22))\n",
    "    y_top = np.argmin(np.abs(y_values - window_height))\n",
    "    y_bottom = np.argmin(np.abs(y_values + window_height))\n",
    "    y_seed_point = np.argmin(np.abs(y_values))\n",
    "\n",
    "    closed = cv2.morphologyEx(return_array, cv2.MORPH_CLOSE, kernel)\n",
    "    closed[y_top, :] = 1\n",
    "    closed[y_bottom, :] = 1\n",
    "\n",
    "    return np.where(closed ==  1, 1 , np.nan), y_seed_point\n",
    "    \n",
    "def draw_sections(edge_array):\n",
    "    edges1, seed_index = edge_array\n",
    "    edges = edges1.copy() \n",
    "\n",
    "    neighbours = [(-1, 0), (0, -1), (0, 1), (1, 0)]  \n",
    "    \n",
    "    nan_mask = np.isnan(edges)  \n",
    "    int = 2\n",
    "\n",
    "    for x in range(edges.shape[1]):\n",
    "\n",
    "        if nan_mask[seed_index, x]: \n",
    "            queue = deque([(seed_index, x)])\n",
    "\n",
    "            while queue:\n",
    "                y, x = queue.popleft()\n",
    "                edges[y, x] = int \n",
    "                nan_mask[y, x] = False  \n",
    "                \n",
    "                neighbors_to_add = [\n",
    "                    (ny, nx) for dy, dx in neighbours\n",
    "                    if (0 <= (ny := y + dy) < edges.shape[0] and \n",
    "                        0 <= (nx := x + dx) < edges.shape[1] and \n",
    "                        nan_mask[ny, nx])  \n",
    "                ]\n",
    "\n",
    "                queue.extend(neighbors_to_add) \n",
    "                for ny, nx in neighbors_to_add:\n",
    "                    nan_mask[ny, nx] = False \n",
    "                    \n",
    "        int += 1\n",
    "    edges = np.where(edges > 1, edges, np.nan)\n",
    "\n",
    "    return edges\n",
    "\n",
    "def label_x_values(data):\n",
    "    print(\"finding edges...\")\n",
    "    edges, seed_index = get_edges(data)\n",
    "    z_array = data[heights]\n",
    "    cz_array = data[convolved_heights]\n",
    "    neighbours = [(-1, 0), (0, -1), (0, 1), (1, 0)]\n",
    "    x_values = np.zeros(z_array.shape[1], dtype=int)\n",
    "    nan_mask = np.isnan(edges) \n",
    "\n",
    "    print(\"classifying sections...\")\n",
    "\n",
    "    for x in range(edges.shape[1]):\n",
    "        center_x_vals = []\n",
    "        avg_z_vals = {}\n",
    "        avg_cz_vals = {}\n",
    "        size = 0\n",
    "        if nan_mask[seed_index, x]: \n",
    "            queue = deque([(seed_index, x)])\n",
    "            while queue:\n",
    "                y, x = queue.popleft()\n",
    "                size += 1\n",
    "\n",
    "                if y == seed_index:\n",
    "                    center_x_vals.append(x)\n",
    "\n",
    "                if y not in avg_z_vals.keys():\n",
    "                    avg_z_vals[y] = []\n",
    "                    avg_cz_vals[y] = []\n",
    "\n",
    "                avg_z_vals[y].append(z_array[y,x])\n",
    "                avg_cz_vals[y].append(cz_array[y,x])\n",
    "                nan_mask[y, x] = False  \n",
    "\n",
    "                neighbors_to_add = [\n",
    "                    (ny, nx) for dy, dx in neighbours\n",
    "                    if (0 <= (ny := y + dy) < edges.shape[0] and \n",
    "                        0 <= (nx := x + dx) < edges.shape[1] and \n",
    "                        nan_mask[ny, nx])  \n",
    "                ]\n",
    "                queue.extend(neighbors_to_add) \n",
    "                for ny, nx in neighbors_to_add:\n",
    "                    nan_mask[ny, nx] = False\n",
    "        \n",
    "        if size > 1000:\n",
    "            avg_z_vals = [np.nanmedian(z_values) for z_values in avg_z_vals.values()]\n",
    "            avg_cz_vals = [np.nanmedian(z_values) for z_values in avg_cz_vals.values()]\n",
    "            label = classify_fhe(avg_z_vals,avg_cz_vals)\n",
    "            for x in center_x_vals:\n",
    "                x_values[x] = label\n",
    "    \n",
    "    return x_values\n",
    "\n",
    "def classify_rubble(data, x_window=1, y_window=4):\n",
    "    pc = cp.array(data[heights])\n",
    "    global results \n",
    "    x_values = data[i_to_x]\n",
    "    y_values = data[i_to_y]\n",
    "    x_value_labels = data[sectioned]\n",
    "    y_center = np.argmin(np.abs(y_values))\n",
    "\n",
    "    pc = pc[y_center - y_window:y_center + y_window + 1,:]\n",
    "\n",
    "    width = pc.shape[1]\n",
    "    y_range, x_range = 2 * y_window + 1, 2 * x_window + 1\n",
    "\n",
    "    x_n = cp.arange(-x_window, x_window + 1) /2   # these values are estimates for the difference between points, they could be improved. \n",
    "    y_n = cp.arange(-y_window, y_window + 1) /4.5 # these values are estimates for the difference between points, they could be improved. \n",
    "    valid_x = cp.repeat(x_n, y_range).flatten()\n",
    "    valid_y = cp.tile(y_n, x_range).flatten()\n",
    "\n",
    "    global points\n",
    "\n",
    "    points = cp.lib.stride_tricks.sliding_window_view(\n",
    "        cp.pad(pc, ((0,0), (x_window,x_window)), mode='edge'), (y_range, x_range)\n",
    "    )\n",
    "\n",
    "    points = points.reshape(width, y_range*x_range)\n",
    "\n",
    "    points = cp.stack((\n",
    "        cp.broadcast_to(valid_x, (width, y_range*x_range)),\n",
    "        cp.broadcast_to(valid_y, (width, y_range*x_range)),\n",
    "        points), axis=2)\n",
    "    mean_vals = cp.mean(points[:,:,2], axis=(1), keepdims=True)\n",
    "    points[:,:,2] -= mean_vals\n",
    "\n",
    "    cov_matrices = cp.matmul(points.transpose(0,2,1), points) / (y_range * x_range - 1)\n",
    "\n",
    "    global eigvecs\n",
    "    \n",
    "    eigvals, eigvecs = cp.linalg.eigh(cov_matrices)\n",
    "\n",
    "    eigvecs = eigvecs[:, :, 0]\n",
    "\n",
    "    eigvals = eigvals[:,0]/(eigvals[:,0] + eigvals[:,1]  + eigvals[:,2]) \n",
    "    \n",
    "    x_offset = cp.abs(cp.arctan(eigvecs[:,0]/eigvecs[:,2]))\n",
    "    y_offset = cp.abs(cp.arctan(eigvecs[:,1]/eigvecs[:,2]))\n",
    "\n",
    "    results = {x_values[i]: x_value_labels[i] if x_value_labels[i] != 0 else [float(eigvals[i]),float(x_offset[i]),float(y_offset[i])] for i in range(len(x_values))}\n",
    "\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    eigvecs = cp.asnumpy(eigvecs)\n",
    "    return results\n",
    "\n",
    "def define_correction_windows(point_cloud, window_size=10):\n",
    "    rubble_dictionary = classify_rubble(point_cloud)  \n",
    "    x_values = np.array(point_cloud[i_to_x]) \n",
    "\n",
    "    global windows\n",
    "    \n",
    "    windows = {}\n",
    "    i = 0\n",
    "\n",
    "    while i < len(x_values):\n",
    "        x_start = x_values[i]\n",
    "        values = [rubble_dictionary[x_start]] \n",
    "\n",
    "        i += 1\n",
    "        while i < len(x_values) and x_values[i] - x_start <= window_size:\n",
    "            values.append(rubble_dictionary[x_values[i]])\n",
    "            i += 1\n",
    "\n",
    "        x_end = x_values[i - 1]  \n",
    "        \n",
    "        labels = np.array([v for v in values if v in [1,2,3]])\n",
    "        rubble_points = [v for v in values if v not in [0,1,2,3]]\n",
    "        \n",
    "        rubble_points = np.array(rubble_points)\n",
    "    \n",
    "        half_perc = np.sum(labels == 1) / len(values)\n",
    "        empty_perc = np.sum(labels == 2) / len(values)\n",
    "        full_perc = np.sum(labels == 3) / len(values)\n",
    "        rubble_perc = 1 - half_perc - empty_perc - full_perc\n",
    "\n",
    "        if rubble_points.size > 0:\n",
    "            avg_var = np.nanmedian(rubble_points[:, 0])\n",
    "            avg_x_offset = np.nanmedian(rubble_points[:, 1])\n",
    "            avg_y_offset = np.nanmedian(rubble_points[:, 2])\n",
    "        else:\n",
    "            avg_var = avg_x_offset = avg_y_offset = np.nan  \n",
    "\n",
    "        # Store results\n",
    "        windows[(x_start, x_end)] = [half_perc, full_perc, empty_perc, rubble_perc, avg_var, avg_x_offset, avg_y_offset]\n",
    "\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parsers\n",
    "class MacassaFileParser:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.box_folders = []\n",
    "\n",
    "    def find_folders(self):\n",
    "        for root, dirs, files in os.walk(self.root_dir):\n",
    "                for dir_name in dirs:\n",
    "                    if dir_name.startswith(\"AdaptiveZ_10mm\"):\n",
    "                        folder = os.path.join(root, dir_name)\n",
    "                        if os.path.isdir(folder): \n",
    "                            for part_folder in os.listdir(folder):\n",
    "                                part_folder = os.path.join(dir_name,part_folder)\n",
    "                                full_part_path = os.path.join(root, part_folder)\n",
    "                                if os.path.isdir(full_part_path) and \"Part\" in part_folder:\n",
    "                                    component_parameters_path = None\n",
    "                                    lidar2xrf_path = None\n",
    "                                    bpc_path = None\n",
    "                                    real_dbg_path = None\n",
    "                                    lidar_times_path = None\n",
    "                                    for file_name in os.listdir(full_part_path):\n",
    "                                        if file_name.endswith(\".component_parameters.txt\"):\n",
    "                                            component_parameters_path = os.path.join(full_part_path, file_name)\n",
    "                                        elif file_name.endswith(\".lidar2xrf\"):\n",
    "                                            lidar2xrf_path = os.path.join(full_part_path, file_name)\n",
    "                                        elif file_name.endswith(\".bpc\"):\n",
    "                                            bpc_path = os.path.join(full_part_path, file_name)\n",
    "                                        elif file_name.endswith(\"_intensity.png\"):\n",
    "                                            intensity_path = os.path.join(full_part_path, file_name)\n",
    "                                        elif file_name.endswith(\"real.dbg\"):\n",
    "                                            real_dbg_path = os.path.join(full_part_path, file_name)\n",
    "                                        elif file_name.endswith(\".dbg\"):\n",
    "                                            lidar_times_path = os.path.join(full_part_path, file_name)\n",
    "\n",
    "                                    self.box_folders.append((full_part_path,component_parameters_path, lidar2xrf_path, bpc_path, intensity_path, real_dbg_path, lidar_times_path))\n",
    "\n",
    "\n",
    "    def get_box_folders(self):\n",
    "        self.find_folders()\n",
    "        return self.box_folders\n",
    "    \n",
    "class BasicParser:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.box_folders = []\n",
    "\n",
    "    def find_folders(self):\n",
    "        for root, dirs, files in os.walk(self.root_dir):\n",
    "                for dir_name in dirs:\n",
    "                    if dir_name.startswith(\"Core\"):\n",
    "                        folder = os.path.join(root,dir_name)\n",
    "                      \n",
    "                        if os.path.isdir(folder) and \"Part\" in folder:\n",
    "                            component_parameters_path = None\n",
    "                            lidar2xrf_path = None\n",
    "                            bpc_path = None\n",
    "                            intensity_path = None\n",
    "                            real_dbg_path = None\n",
    "                            lidar_times_path = None\n",
    "                            for file_name in os.listdir(folder):\n",
    "                                if file_name.endswith(\".component_parameters.txt\"):\n",
    "                                    component_parameters_path = os.path.join(folder, file_name)\n",
    "                                elif file_name.endswith(\".lidar2xrf\"):\n",
    "                                    lidar2xrf_path = os.path.join(folder, file_name)\n",
    "                                elif file_name.endswith(\".bpc\"):\n",
    "                                    bpc_path = os.path.join(folder, file_name)\n",
    "                                elif file_name.endswith(\"_intensity.png\"):\n",
    "                                    intensity_path = os.path.join(folder, file_name)\n",
    "                                elif file_name.endswith(\"real.dbg\"):\n",
    "                                    real_dbg_path = os.path.join(folder, file_name)\n",
    "                                elif file_name.endswith(\".dbg\"):\n",
    "                                    lidar_times_path = os.path.join(folder, file_name)\n",
    "\n",
    "                            self.box_folders.append((folder,component_parameters_path, lidar2xrf_path, bpc_path, intensity_path, real_dbg_path, lidar_times_path))\n",
    "\n",
    "\n",
    "    def get_box_folders(self):\n",
    "        self.find_folders()\n",
    "        return self.box_folders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose and use parser\n",
    "parser = MacassaFileParser(part_path)\n",
    "#parser = BasicParser(r\"\\\\192.168.1.100\\CoreScan3-2\\Acquisitions\\RnD\\XRF 2.0\\same core scans\\AI21-TarcoreDoubleScans old xrf head\\AI21-XRF1.0\")\n",
    "paths_list = parser.get_box_folders()\n",
    "\n",
    "for name, component_parameters_path, lidar2xrf_path, bpc_path, intensity_path, real_dbg_path, lidar_times_path  in paths_list:\n",
    "    print(f\"Part: {name}\")\n",
    "    print(f\"  Component Parameters: {component_parameters_path}\")\n",
    "    print(f\"  LIDAR to XRF: {lidar2xrf_path}\")\n",
    "    print(f\"  BPC File: {bpc_path}\")\n",
    "    print(f\"  Intensity File: {intensity_path}\")\n",
    "    print(f\"  lidar time stamps file: {lidar_times_path}\")\n",
    "    print(f\"  real dbg file: {real_dbg_path}\")\n",
    "\n",
    "valid = np.sum([all(item is not None for item in sublist) for sublist in paths_list])\n",
    "\n",
    "print(f\"{len(paths_list)} part paths found\")\n",
    "print(f\"{valid} valid paths found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the point clouds\n",
    "point_clouds = []\n",
    "\n",
    "for paths in paths_list: \n",
    "   if None not in paths:\n",
    "        point_clouds.append(get_point_cloud(paths, upsample_ratio=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the label tree\n",
    "with open('cluster_kd_tree.pkl', 'rb') as f:\n",
    "    tree, labels = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for point_cloud in point_clouds:\n",
    "    if len(point_cloud) > sectioned:\n",
    "        point_cloud[sectioned] = label_x_values(point_cloud)\n",
    "    else:\n",
    "        point_cloud.append(label_x_values(point_cloud))\n",
    "    print(f\"done {point_cloud[0]} .\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_labels:\n",
    "    fig = plt.figure(figsize=(70, 1.5 * len(point_clouds)), dpi=150)\n",
    "    gs = fig.add_gridspec(len(point_clouds), 1, hspace=0.025)\n",
    "\n",
    "    colormap = [\"red\", \"green\", \"blue\", \"purple\"]\n",
    "\n",
    "    for i, pc in enumerate(point_clouds):\n",
    "        middle_y = np.argmin(np.abs(point_cloud[i_to_y]))\n",
    "        ax = fig.add_subplot(gs[i, 0])\n",
    "        display = pc[intensity]\n",
    "        ax.imshow(np.flipud(display), cmap='bone', interpolation='nearest', alpha=1)\n",
    "\n",
    "        x_vals = np.arange(display.shape[1])\n",
    "        labels = np.array(list(pc[sectioned]))\n",
    "\n",
    "        colors = np.array([colormap[label] for label in labels])\n",
    "\n",
    "        ds_f = 4\n",
    "        x_vals = x_vals[::ds_f]\n",
    "        colors = colors[::ds_f]\n",
    "\n",
    "        ax.bar(x_vals, height=8, width=ds_f, bottom=middle_y - 4, color=colors, alpha=0.85)\n",
    "\n",
    "        ax.set_xlabel(\"X index\")\n",
    "        ax.set_ylabel(\"Y index\")\n",
    "        ax.set_title(pc[label], fontsize=15)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_sections:\n",
    "    section_displays = []\n",
    "\n",
    "    for point_cloud in point_clouds:\n",
    "        edges = get_edges(point_cloud,s = 80, w = 30, y_window=9, x_window=2, s_e = 0.01, w_e = 0.0075)\n",
    "        sections = draw_sections(edges)\n",
    "        section_displays.append(sections)\n",
    "        print(f\"done {point_cloud[0]}\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(70, 1.5*len(point_clouds)), dpi=100)\n",
    "    gs = fig.add_gridspec(len(point_clouds), 1, hspace=0.025)\n",
    "\n",
    "\n",
    "    for (i,pc) , (i2, edges) in zip(enumerate(point_clouds), enumerate(section_displays)):\n",
    "        ax = fig.add_subplot(gs[i, 0])\n",
    "        \n",
    "        ax.imshow(np.flipud(pc[heights]), cmap='bone_r', interpolation='nearest', alpha = 1)\n",
    "        ax.imshow(cp.flipud(pc[intensity]), cmap='bone', interpolation='nearest', alpha = 0.65)\n",
    "        ax.imshow(cp.flipud(edges), cmap='nipy_spectral', interpolation='nearest', alpha = 0.65)\n",
    "        ax.set_xlabel(\"X index\")\n",
    "        ax.set_ylabel(\"Y index\")\n",
    "        ax.set_title(pc[0],fontsize = 20)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for point_cloud in point_clouds:\n",
    "    if len(point_cloud) > final:\n",
    "        point_cloud[final] = define_correction_windows(point_cloud)\n",
    "    else:\n",
    "        point_cloud.append(define_correction_windows(point_cloud))\n",
    "    print(f\"done {point_cloud[0]} .\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_rubble_sections:\n",
    "    fig = plt.figure(figsize=(70, 2 * len(point_clouds)), dpi=55)\n",
    "    gs = fig.add_gridspec(len(point_clouds), 1, hspace=0.075)\n",
    "\n",
    "    colormap = [\"green\", \"blue\", \"purple\", \"red\"]\n",
    "    cmap1 = plt.cm.cool  \n",
    "    cmap2 = plt.cm.cool\n",
    "\n",
    "    all_vars = []\n",
    "    all_angles = []\n",
    "\n",
    "    for pc in point_clouds:\n",
    "        data = pc[final]  \n",
    "        for values in data.values():\n",
    "            var, x_angle, y_angle = values[4:7]\n",
    "            all_vars.append(var)\n",
    "            all_angles.append(x_angle)\n",
    "\n",
    "    min_var = np.nanmin(all_vars)\n",
    "    max_var = np.nanmax(all_vars)\n",
    "    min_angle = np.nanmin(all_angles)\n",
    "    max_angle = np.nanmax(all_angles)\n",
    "\n",
    "    print(f\"Min variance: {min_var}\")\n",
    "    print(f\"Max variance: {max_var}\")\n",
    "    print(f\"Min angle: {min_angle}\")\n",
    "    print(f\"Max angle: {max_angle}\")\n",
    "\n",
    "    for i, pc in enumerate(point_clouds):\n",
    "        middle_y = np.argmin(np.abs(point_cloud[i_to_y]))\n",
    "        index_dictionary = pc[x_to_i]\n",
    "        ax = fig.add_subplot(gs[i, 0])\n",
    "        display = pc[intensity]\n",
    "        ax.imshow(np.flipud(display), cmap='bone_r', interpolation='nearest', alpha=1)\n",
    "\n",
    "        data = pc[final]  \n",
    "\n",
    "        for (x_start, x_end), values in data.items():\n",
    "            half_perc, empty_perc, full_perc, rubble_perc = values[:4]\n",
    "            \n",
    "            ratios = np.array([half_perc, empty_perc, full_perc, rubble_perc])\n",
    "            ratios /= ratios.sum() \n",
    "            \n",
    "            width = index_dictionary[x_end] - index_dictionary[x_start] - 1\n",
    "            \n",
    "            bar_heights = ratios * 20 \n",
    "\n",
    "            bottoms = middle_y - 10 + np.insert(np.cumsum(bar_heights[:-1]), 0, 0)\n",
    "\n",
    "            for j in range(4):\n",
    "                ax.bar(index_dictionary[x_start] + 10, height=bar_heights[j], width=width, bottom=bottoms[j], color=colormap[j], alpha=0.65)\n",
    "            if rubble_perc > 0:\n",
    "                var, x_angle, y_angle = values[4:]\n",
    "\n",
    "                norm1 = plt.Normalize(vmin=min_var, vmax=max_var)\n",
    "                norm2 = plt.Normalize(vmin=min_angle, vmax=max_angle)\n",
    "\n",
    "                color1 = cmap1(norm1(var))\n",
    "                color2 = cmap2(norm2(x_angle))\n",
    "                color3 = cmap2(norm2(y_angle))\n",
    "\n",
    "                extra_heights = 15\n",
    "                ax.bar(index_dictionary[x_start] + 10, height=extra_heights, width=width, \n",
    "                       bottom=0, color=color1, alpha=1)\n",
    "                ax.bar(index_dictionary[x_start] + 10, height=extra_heights, width=width, \n",
    "                       bottom=15, color=color2, alpha=1)\n",
    "                ax.bar(index_dictionary[x_start] + 10, height=extra_heights, width=width, \n",
    "                   bottom=30, color=color3, alpha=1)\n",
    "\n",
    "        sm1 = plt.cm.ScalarMappable(cmap=cmap1, norm=norm1)\n",
    "        sm2 = plt.cm.ScalarMappable(cmap=cmap2, norm=norm2)\n",
    "        sm3 = plt.cm.ScalarMappable(cmap=cmap2, norm=norm2)\n",
    "\n",
    "        cbar1 = plt.colorbar(sm1, ax=ax, orientation='vertical', fraction=0.03, pad=0.002)\n",
    "        cbar2 = plt.colorbar(sm2, ax=ax, orientation='vertical', fraction=0.03, pad=0.004)\n",
    "        cbar3 = plt.colorbar(sm3, ax=ax, orientation='vertical', fraction=0.03, pad=0.008)\n",
    "\n",
    "        cbar1.set_label(\"variance\")\n",
    "        cbar2.set_label(\"x_angle\")\n",
    "        cbar3.set_label(\"y_angle\")\n",
    "\n",
    "        ax.set_xlabel(\"X index\")\n",
    "        ax.set_ylabel(\"Y index\")\n",
    "        ax.set_title(pc[label], fontsize=15)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dont run this part\n",
    "\n",
    "for point_cloud in point_clouds:\n",
    "    if len(point_cloud) > final:\n",
    "        folder = point_cloud[0] \n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        \n",
    "        file_path = os.path.join(folder, 'rubble_classification.pkl')\n",
    "        \n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(point_cloud[final], f)\n",
    "\n",
    "        print(f\"Saved to {file_path}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env2",
   "language": "python",
   "name": "new_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
