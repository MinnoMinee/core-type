{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import hdbscan\n",
    "from scipy.stats import skew, kurtosis, mode\n",
    "from scipy.spatial import KDTree\n",
    "import json\n",
    "import pickle\n",
    "from hdbscan.prediction import approximate_predict\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parts(directory_path):\n",
    "    parts_paths = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for folder in dirs:\n",
    "            folder = os.path.join(root,folder)\n",
    "            if (\"Box1\\AdaptiveZ_10mm\" in folder) and folder.endswith(\"_4\"):\n",
    "                parts_paths.append(folder)\n",
    "    parts_paths.sort(reverse=True)\n",
    "    return parts_paths\n",
    "\n",
    "base_path = r\"\\\\192.168.1.100\\CoreScan3-2\\Acquisitions\\RnD\\XRF\\CH\\Macassa_clearance\"\n",
    "\n",
    "paths = find_parts(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_point_cloud(file_path):\n",
    "    coords_file = os.path.join(file_path, '.component_parameters.txt')\n",
    "    with open(coords_file) as file:\n",
    "            lines = file.readlines()\n",
    "                \n",
    "            for line in lines:\n",
    "                if \"XRAY_DPP[Acquisition]#0.Y.Start:\" in line:\n",
    "                    y_offset = (float)(line.split(\"XRAY_DPP[Acquisition]#0.Y.Start:\")[1].strip())\n",
    "                elif \"XRAY_DPP[Acquisition]#0.X.Start:\" in line:\n",
    "                    x_start = (float)(line.split(\"XRAY_DPP[Acquisition]#0.X.Start:\")[1].strip())\n",
    "                elif \"XRAY_DPP[Acquisition]#0.X.Stop:\" in line:\n",
    "                    x_stop = (float)(line.split(\"XRAY_DPP[Acquisition]#0.X.Stop:\")[1].strip())\n",
    "\n",
    "\n",
    "    point_cloud = []\n",
    "\n",
    "    if os.path.isdir(file_path):\n",
    "        lidar_files = [fn for fn in os.listdir(\n",
    "            file_path) if fn.endswith('.bpc')]\n",
    "        if any(lidar_files):\n",
    "            lidar_filename = file_path + os.sep + lidar_files[0]\n",
    "\n",
    "    data = np.fromfile(lidar_filename, dtype=np.float32)\n",
    "    point_cloud = data.reshape(-1, 3)  # to xyz\n",
    "\n",
    "    ff = ~np.isnan(point_cloud).any(axis=1)\n",
    "    point_cloud = point_cloud[ff, ...]\n",
    "\n",
    "    point_cloud[:, 1] = point_cloud[:, 1] - float(y_offset)\n",
    "\n",
    "    print(f\"{file_path} is loaded. \\n# of point {point_cloud.shape[0]}\")\n",
    "\n",
    "\n",
    "    matrix_file = (os.path.join(file_path, \".XRAY_DPP_001.lidar2xrf\"))\n",
    "    with open(matrix_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    transformation_matrix = np.array([list(map(float, line.strip().split(\",\"))) for line in lines])\n",
    "\n",
    "    num_points = point_cloud.shape[0]\n",
    "\n",
    "    homogeneous_points = np.hstack((point_cloud, np.ones((num_points, 1))))\n",
    "    transformed_points = homogeneous_points @ transformation_matrix.T\n",
    "    point_cloud = transformed_points[:, :3]\n",
    "\n",
    "\n",
    "    def trim_cloud(data):\n",
    "        floor = mode(data[:, 2])[0] - 10\n",
    "        print(floor)\n",
    "        #data[:,2] = floor - data[:, 2] \n",
    "        data = data[data[:, 2] > 0]\n",
    "        data = data[\n",
    "        (data[:,0] >= x_stop) & \n",
    "        (data[:,0] <= x_start) \n",
    "        ]\n",
    "        return data\n",
    "    \n",
    "    def remove_y_offset(data):\n",
    "        data[:, 1] -= y_offset\n",
    "        return data \n",
    "\n",
    "    point_cloud = trim_cloud(point_cloud)\n",
    "    point_cloud = remove_y_offset(point_cloud)\n",
    "    return point_cloud, x_start, x_stop\n",
    "\n",
    "def trim_y(data, y_span=20):\n",
    "   data = data[(data[:,1] >= -y_span) & (data[:,1] <= y_span)]\n",
    "   return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(data, n=100000):\n",
    "    downsampled_indices = np.random.choice(data.shape[0], size=n, replace=False)\n",
    "\n",
    "    x = data[:, 0]\n",
    "    y = data[:, 1]\n",
    "    z = data[:, 2]\n",
    "\n",
    "    downsampled = np.zeros((n, 3))\n",
    "\n",
    "    downsampled[:, 0] = x[downsampled_indices]\n",
    "    downsampled[:, 1] = y[downsampled_indices]\n",
    "    downsampled[:, 2] = z[downsampled_indices]\n",
    "    return downsampled\n",
    "\n",
    "point_clouds = []\n",
    "x_offset = 0\n",
    "for path in paths:\n",
    "    temp_cloud, x_start, x_stop = get_point_cloud(path)\n",
    "    x_offset -= x_stop\n",
    "    temp_cloud[:,0] += x_offset\n",
    "    x_offset += x_start\n",
    "    point_clouds.append(temp_cloud)\n",
    "   \n",
    "point_cloud = np.vstack(point_clouds)\n",
    "print(f\"total # of point {point_cloud.shape[0]}\")\n",
    "point_cloud = trim_y(point_cloud,20)\n",
    "\n",
    "plot_cloud = point_cloud.copy()\n",
    "print(f\"total # of point {point_cloud.shape[0]}\")\n",
    "print(len(np.unique(point_cloud[:,0])))\n",
    "\n",
    "point_tree = KDTree(point_cloud[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(x_span = 1, y_span = 12, step = 1,y_index = 0):\n",
    "    vectors = {}\n",
    "    x_values = np.unique(point_cloud[:, 0])\n",
    "    y = y_index\n",
    "    x = x_values.min()\n",
    "    while(x < x_values.max()):\n",
    "        distribution = get_distribution([x, y], x_span, y_span) \n",
    "        properties = get_props(distribution)\n",
    "        if not np.any(np.isnan(properties)):\n",
    "            vectors[x] = properties\n",
    "        x += step\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def get_distribution(point=[0, 0], x_span=10, y_span=15):\n",
    "    search_radius = max(x_span, y_span)\n",
    "\n",
    "    indices = point_tree.query_ball_point(point, search_radius)\n",
    "\n",
    "    result_points = point_cloud[indices]\n",
    "\n",
    "    filtered_points = result_points[\n",
    "        (result_points[:, 0] >= point[0] - x_span) & (result_points[:, 0] <= point[0] + x_span) &\n",
    "        (result_points[:, 1] >= point[1] - y_span) & (result_points[:, 1] <= point[1] + y_span)\n",
    "    ]\n",
    "\n",
    "    return filtered_points[:, 2]\n",
    "\n",
    "\n",
    "def get_distribution_vs_y(point=[0, 0], x_span=10, y_span=15):\n",
    "    search_radius = max(x_span, y_span)\n",
    "\n",
    "    indices = point_tree.query_ball_point(point, search_radius)\n",
    "\n",
    "    result_points = point_cloud[indices]\n",
    "\n",
    "    filtered_points = result_points[\n",
    "        (result_points[:, 0] >= point[0] - x_span) & (result_points[:, 0] <= point[0] + x_span) &\n",
    "        (result_points[:, 1] >= point[1] - y_span) & (result_points[:, 1] <= point[1] + y_span)\n",
    "    ]\n",
    "\n",
    "    y_vals = {}\n",
    "\n",
    "    for y in range(len(filtered_points[:,1])):\n",
    "        if filtered_points[y,1] not in y_vals:\n",
    "            y_vals[filtered_points[y,1]] = []\n",
    "        y_vals[filtered_points[y,1]].append(filtered_points[y,2])\n",
    "\n",
    "    return y_vals\n",
    "\n",
    "\n",
    "def get_props(distribution):\n",
    "    properties = []\n",
    "\n",
    "    mean = np.mean(distribution)\n",
    "    variance =  np.var(distribution)\n",
    "    skw = skew(distribution)\n",
    "    kurt = kurtosis(distribution)\n",
    "\n",
    "    \n",
    "    properties.append(variance)\n",
    "    properties.append(skw)\n",
    "    properties.append(kurt)\n",
    "\n",
    "    norm = np.sum(x**2 for x in properties)**0.5\n",
    "    \n",
    "    properties = [x / norm for x in properties]\n",
    "\n",
    "    \n",
    "    properties.append(mean)\n",
    "\n",
    "    z = properties[0]\n",
    "    y = properties[1]\n",
    "    x = properties[2]\n",
    "\n",
    "    roe = np.sqrt(x**2 + y**2 + z**2)\n",
    "    properties.append((np.atan2(y,x) + (2 * np.pi)) % (2 * np.pi))\n",
    "    properties.append(np.acos(z/roe)/(np.pi/2))\n",
    "\n",
    "    \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_span = 1\n",
    "y_span = 12\n",
    "step = 10\n",
    "\n",
    "\n",
    "# 0 = variance, 1 = skew, 2 = kurtosis, 3 = mean, 4 = azumithol, 5 = polar\n",
    "properties = [1,5] \n",
    "#cluster size, sample size, metric, epsilon, alpha, max eps\n",
    "parameters = []\n",
    "\n",
    "\n",
    "parameters.append([30, 10, 'chebyshev', 0.06, 0.25, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(19,3))\n",
    "point_cloud_copy = plot_cloud[(plot_cloud[:,1] >= 0 - y_span) & (plot_cloud[:,1] <= y_span)]\n",
    "\n",
    "downsampled_indices = np.random.choice(point_cloud_copy.shape[0], size=50000, replace=False)\n",
    "\n",
    "x = point_cloud_copy[:, 0] \n",
    "y = point_cloud_copy[:, 1] \n",
    "z = point_cloud_copy[:, 2] \n",
    "\n",
    "x_downsampled = x[downsampled_indices]\n",
    "y_downsampled = y[downsampled_indices]\n",
    "z_downsampled = z[downsampled_indices]\n",
    "\n",
    "\n",
    "gs = fig.add_gridspec(len(parameters)+1, 2, hspace = 0.8, wspace = 0.3, width_ratios = [8,1] )\n",
    "\n",
    "ax_scatter = fig.add_subplot(gs[0,0])\n",
    "ax_scatter.scatter(\n",
    "    x_downsampled,\n",
    "    y_downsampled,\n",
    "    c=z_downsampled,\n",
    "    cmap='viridis',  \n",
    "    s = 1\n",
    ")\n",
    "\n",
    "ax_scatter.set_title('LIDAR data heatmap')\n",
    "ax_scatter.set_xlabel('X-axis')\n",
    "ax_scatter.set_ylabel('Y-axis')\n",
    "\n",
    "\n",
    "\n",
    "v_dict = get_vectors(x_span,y_span,step)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for param in parameters:\n",
    "    count += 1\n",
    "    dbscan = hdbscan.HDBSCAN(min_cluster_size = param[0],\n",
    "                            min_samples = param[1], \n",
    "                            metric = param[2], \n",
    "                            cluster_selection_epsilon= param[3],\n",
    "                            alpha = param[4],\n",
    "                            core_dist_n_jobs= -1,\n",
    "                            cluster_selection_method='eom',\n",
    "                            cluster_selection_epsilon_max = param[5])\n",
    "\n",
    "\n",
    "    v_array = np.array(list(v_dict.values()))\n",
    "    x_values = list(v_dict.keys())\n",
    "\n",
    "    labels = dbscan.fit_predict(v_array[:, properties])\n",
    "\n",
    "    \n",
    "    initial_clusters = labels\n",
    "    refined_clusters = []\n",
    "\n",
    "\n",
    "\n",
    "    cluster_points = v_array[initial_clusters == 1]\n",
    "    \n",
    "    hdbscan_refined = hdbscan.HDBSCAN(min_cluster_size = 5,\n",
    "                            min_samples = 1, \n",
    "                            metric = 'euclidean', \n",
    "                            cluster_selection_epsilon= 0.7,\n",
    "                            alpha = 9.0,\n",
    "                            core_dist_n_jobs= -1,\n",
    "                            cluster_selection_method='eom',\n",
    "                            cluster_selection_epsilon_max = 0.8)\n",
    "    \n",
    "    refined_labels = hdbscan_refined.fit_predict(cluster_points[:, [5]])*2 + 1\n",
    "    \n",
    "    refined_clusters.append(refined_labels)\n",
    "\n",
    "\n",
    "    cluster_indices = np.where(initial_clusters == 1)[0]\n",
    "\n",
    "    for i, idx in enumerate(cluster_indices):\n",
    "        labels[idx] = refined_labels[i]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    bar_plot = fig.add_subplot(gs[count,0])\n",
    "    added_labels = set()\n",
    "\n",
    "    for x, core_type in zip(x_values, labels):\n",
    "        label = f\"Core Type {core_type}\" if core_type not in added_labels else None\n",
    "        if label:\n",
    "            added_labels.add(core_type)\n",
    "        bar_plot.bar(x, height=1, width=step*2, color=f\"C{core_type + 2}\" if core_type >= 0 else \"black\", edgecolor=\"none\", label=label)\n",
    "\n",
    "\n",
    "    bar_plot.set_title(\"Core Type by X-position\")\n",
    "    bar_plot.set_xlabel(\"X-Value\")\n",
    "    bar_plot.legend(title=\"Core Type\", bbox_to_anchor=(1,2), loc=\"upper left\")\n",
    "    bar_plot.set_yticks([])\n",
    "\n",
    "    ax = fig.add_subplot(gs[count,1])\n",
    "    \n",
    "\n",
    "    for vec, label in zip(v_dict.values(), labels):\n",
    "        color = f\"C{label + 2}\" if label >= 0 else \"black\"\n",
    "\n",
    "        ax.scatter(vec[1], vec[5], color=color, s=1)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#with open('hdbscan_1.pkl', 'wb') as f:\n",
    "#    pickle.dump(dbscan, f)\n",
    "#\n",
    "#with open('hdbscan_2.pkl', 'wb') as f:\n",
    "#    pickle.dump(hdbscan_refined, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_span = 1\n",
    "y_span = 12\n",
    "step = 10\n",
    "\n",
    "\n",
    "# 0 = variance, 1 = skew, 2 = kurtosis, 3 = mean, 4 = azumithol, 5 = polar\n",
    "properties = [1,5] \n",
    "#cluster size, sample size, metric, epsilon, alpha, max eps\n",
    "parameters = []\n",
    "\n",
    "\n",
    "parameters.append([11, 'chebyshev', 0.06])\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(19,3))\n",
    "point_cloud_copy = plot_cloud[(plot_cloud[:,1] >= 0 - y_span) & (plot_cloud[:,1] <= y_span)]\n",
    "\n",
    "downsampled_indices = np.random.choice(point_cloud_copy.shape[0], size=50000, replace=False)\n",
    "\n",
    "x = point_cloud_copy[:, 0] \n",
    "y = point_cloud_copy[:, 1] \n",
    "z = point_cloud_copy[:, 2] \n",
    "\n",
    "x_downsampled = x[downsampled_indices]\n",
    "y_downsampled = y[downsampled_indices]\n",
    "z_downsampled = z[downsampled_indices]\n",
    "\n",
    "\n",
    "gs = fig.add_gridspec(len(parameters)+1, 2, hspace = 0.8, wspace = 0.3, width_ratios = [8,1] )\n",
    "\n",
    "ax_scatter = fig.add_subplot(gs[0,0])\n",
    "ax_scatter.scatter(\n",
    "    x_downsampled,\n",
    "    y_downsampled,\n",
    "    c=z_downsampled,\n",
    "    cmap='viridis',  \n",
    "    s = 1\n",
    ")\n",
    "\n",
    "ax_scatter.set_title('LIDAR data heatmap')\n",
    "ax_scatter.set_xlabel('X-axis')\n",
    "ax_scatter.set_ylabel('Y-axis')\n",
    "\n",
    "\n",
    "\n",
    "v_dict = get_vectors(x_span,y_span,step)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for param in parameters:\n",
    "    count += 1\n",
    "    dbscan = DBSCAN(min_samples = param[0], \n",
    "                    metric = param[1], \n",
    "                    eps= param[2],\n",
    "                    n_jobs= -1)\n",
    "\n",
    "\n",
    "    v_array = np.array(list(v_dict.values()))\n",
    "    x_values = list(v_dict.keys())\n",
    "\n",
    "    labels = dbscan.fit_predict(v_array[:, properties])\n",
    "\n",
    "    \n",
    "    initial_clusters = labels\n",
    "    refined_clusters = []\n",
    "\n",
    "\n",
    "\n",
    "    cluster_points = v_array[initial_clusters == 1]\n",
    "    \n",
    "    dbscan_refined = DBSCAN(min_samples = 5, \n",
    "                             metric = \"euclidean\", \n",
    "                             eps= 0.06,\n",
    "                             n_jobs= -1)\n",
    "\n",
    "\n",
    "    refined_labels = hdbscan_refined.fit_predict(cluster_points[:, [4]])*2 + 1\n",
    "    \n",
    "    refined_clusters.append(refined_labels)\n",
    "\n",
    "    cluster_indices = np.where(initial_clusters == 1)[0]\n",
    "\n",
    "    for i, idx in enumerate(cluster_indices):\n",
    "        labels[idx] = refined_labels[i]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    bar_plot = fig.add_subplot(gs[count,0])\n",
    "    added_labels = set()\n",
    "\n",
    "    for x, core_type in zip(x_values, labels):\n",
    "        label = f\"Core Type {core_type}\" if core_type not in added_labels else None\n",
    "        if label:\n",
    "            added_labels.add(core_type)\n",
    "        bar_plot.bar(x, height=1, width=step*2, color=f\"C{core_type + 2}\" if core_type >= 0 else \"black\", edgecolor=\"none\", label=label)\n",
    "\n",
    "\n",
    "    bar_plot.set_title(\"Core Type by X-position\")\n",
    "    bar_plot.set_xlabel(\"X-Value\")\n",
    "    bar_plot.legend(title=\"Core Type\", bbox_to_anchor=(1,2), loc=\"upper left\")\n",
    "    bar_plot.set_yticks([])\n",
    "\n",
    "    ax = fig.add_subplot(gs[count,1])\n",
    "\n",
    "    core_points = [v_dict[key] for i, key in enumerate(v_dict) if i in dbscan.core_sample_indices_]\n",
    "    labels_core_points = labels[dbscan.core_sample_indices_]\n",
    "\n",
    "    #downsampled_indices = np.random.choice(len(core_points), size=600, replace=False)\n",
    "    #\n",
    "    #core_points = np.array(core_points)[downsampled_indices]\n",
    "    #labels_core_points = labels_core_points[downsampled_indices]\n",
    "\n",
    "    for vec, label in zip(core_points, labels_core_points):\n",
    "        color = f\"C{label + 2}\" if label >= 0 else \"black\"\n",
    "\n",
    "        ax.scatter(vec[1], vec[5], color=color, s=1)\n",
    "print(len(core_points))\n",
    "print(len(v_dict.values()))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "with open('hdbscan_1.pkl', 'wb') as f:\n",
    "    pickle.dump(dbscan, f)\n",
    "\n",
    "with open('hdbscan_2.pkl', 'wb') as f:\n",
    "    pickle.dump(dbscan_refined, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "\n",
    "hdbscan_refined = DBSCAN(min_samples = 5, \n",
    "                             metric = \"euclidean\", \n",
    "                             eps= 0.06,\n",
    "                             n_jobs= -1)\n",
    "\n",
    "\n",
    "refined_labels = hdbscan_refined.fit_predict(cluster_points[:, [0,1,2]])*2  +1\n",
    "\n",
    "refined_clusters.append(refined_labels)\n",
    "for i, idx in enumerate(cluster_indices):\n",
    "    labels[idx] = refined_labels[i]\n",
    "\n",
    "r2_labels = labels[cluster_indices]\n",
    "\n",
    "added_labels = set()\n",
    "for vec, label in zip(cluster_points, r2_labels):\n",
    "    color = f\"C{label + 2}\" if label >= 0 else \"black\"\n",
    "    ax.scatter(vec[0],vec[1],vec[2], color=color, s=1)\n",
    "\n",
    "\n",
    "ax.set_title(\"3D Scatter of Vectors by Core Type\", pad=20)\n",
    "ax.set_xlabel(\"0\")\n",
    "ax.set_ylabel(\"1\")\n",
    "ax.set_zlabel(\"2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_span = 1\n",
    "y_span = 12\n",
    "step = 1\n",
    "core_points = np.array(core_points)\n",
    "\n",
    "c_points = core_points[:, [1, 5]]\n",
    "c_type = labels_core_points\n",
    "\n",
    "type_dict = {i: c_type[i] for i in range(len(c_points))}\n",
    "\n",
    "tree = KDTree(c_points)\n",
    "\n",
    "v_dict = get_vectors(x_span, y_span, step)\n",
    "\n",
    "indices = [tree.query([v[1], v[5]], k=1, distance_upper_bound=0.1)[1] for v in v_dict.values()]\n",
    "\n",
    "labels = np.array([type_dict[index] if index != len(c_points) else -1 for index in indices])\n",
    "\n",
    "prediction = {x: label for x, label in zip(v_dict.keys(), labels)}\n",
    "\n",
    "x_vals = [v_dict[x][1] for x in v_dict.keys()]\n",
    "y_vals = [v_dict[x][5] for x in v_dict.keys()]\n",
    "colors = [f\"C{prediction[x] + 2}\" if prediction[x] >= 0 else \"black\" for x in v_dict.keys()]\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(19,3))\n",
    "point_cloud_copy = plot_cloud[(plot_cloud[:,1] >= 0 - y_span) & (plot_cloud[:,1] <= y_span)]\n",
    "\n",
    "downsampled_indices = np.random.choice(point_cloud_copy.shape[0], size=50000, replace=False)\n",
    "\n",
    "x = point_cloud_copy[:, 0] \n",
    "y = point_cloud_copy[:, 1] \n",
    "z = point_cloud_copy[:, 2] \n",
    "\n",
    "x_downsampled = x[downsampled_indices]\n",
    "y_downsampled = y[downsampled_indices]\n",
    "z_downsampled = z[downsampled_indices]\n",
    "\n",
    "\n",
    "gs = fig.add_gridspec(2, 2, hspace = 0.5, wspace = 0.2, width_ratios = [9,1] )\n",
    "\n",
    "ax_scatter = fig.add_subplot(gs[0,0])\n",
    "ax_scatter.scatter(\n",
    "    x_downsampled,\n",
    "    y_downsampled,\n",
    "    c=z_downsampled,\n",
    "    cmap='viridis',  \n",
    "    s = 1\n",
    ")\n",
    "\n",
    "ax_scatter.set_title('LIDAR data heatmap')\n",
    "ax_scatter.set_xlabel('X-axis')\n",
    "ax_scatter.set_ylabel('Y-axis')\n",
    "  \n",
    "bar_plot = fig.add_subplot(gs[1,0])\n",
    "\n",
    "bar_plot.bar(v_dict.keys(), height=1, width=step*2, color=colors, edgecolor=\"none\")\n",
    "\n",
    "bar_plot.set_title(\"Core Type by X-position\")\n",
    "bar_plot.set_xlabel(\"X-Value\")\n",
    "bar_plot.set_yticks([])\n",
    "\n",
    "ax = fig.add_subplot(gs[:,1])\n",
    "\n",
    "ax.scatter(x_vals, y_vals, color=colors, s=1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
